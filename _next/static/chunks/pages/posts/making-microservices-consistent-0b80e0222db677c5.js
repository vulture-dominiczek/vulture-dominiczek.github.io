(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[500],{7526:function(e,t,s){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/making-microservices-consistent",function(){return s(5728)}])},5728:function(e,t,s){"use strict";s.r(t);var i=s(5893),o=s(9008);t.default=function(){return(0,i.jsx)("div",{children:(0,i.jsxs)("article",{className:"md:w-3/5 text-lg mr-auto ml-auto",children:[(0,i.jsxs)(o.default,{children:[(0,i.jsx)("title",{children:"Making microservices consistent"}),(0,i.jsx)("meta",{name:"Description",content:"Post explains fault-tolerant systems, CAP theorem and details of Lamport's Paxos protocol."})]}),(0,i.jsxs)("div",{className:"p-14",children:[(0,i.jsx)("h1",{className:"text-3xl font-bold text-center pb-14",children:"Making microservices consistent"}),(0,i.jsxs)("p",{className:"mt-14",children:["Probably everybody has heard about microservices, as presently ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://www.oreilly.com/radar/microservices-adoption-in-2020/",children:(0,i.jsx)("b",{children:"80% of the market"})})," already moved to them and time of ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://en.wikipedia.org/wiki/Technology_adoption_life_cycle#/media/File:DiffusionOfInnovation.png",children:(0,i.jsx)("b",{children:"monolith"})})," seems to be long gone. Long story short, they are important ingredient to any application, because they solve scalability issues. If you found a bottleck in the code just cut it out to separate service and you can have as many instances as you like. Beside that, they allow companies to evolve smoothly. Inside single service you have much more freedom in experimenting with conventions, internal logic or even the choice of language."]}),(0,i.jsx)("p",{className:"pt-7",children:"But while all this is true, microservices are also a source of different kinds of problems... Services can fail. Network connections can be down also. Now components talk to each other in non-obvious ways in less reliable manner. Sometimes conflicts arise and they degrade either performance or functionality or even both..."}),(0,i.jsxs)("p",{className:"pt-7",children:["In this post I wanted to recap some theory of distributed systems. First we start with ",(0,i.jsx)("b",{children:"CAP theorem"})," on imaginary example, a most basic building block for reasoning about distributed systems. Then I  summarize Paxos protocol for reaching ",(0,i.jsx)("b",{children:"consensus"})," among the nodes."]}),(0,i.jsx)("h2",{className:"text-2xl pt-14 py-8 font-bold",children:"What is fault tolerance?"}),(0,i.jsx)("p",{children:"First things first... The service has always inherent risk of going down. It depends on many factors, such as network quality, pressure on the system (cpu load), complexity of logic or communication patterns and many others."}),(0,i.jsxs)("p",{className:"pt-5",children:["How do we build systems to minimize this drawback? In engineering field, component that responds to failure with minimal damages to other (dependant ones) is called ",(0,i.jsx)("b",{children:"fail-safe"}),". As an example, if elevator cables break, this triggers brakes to automatically latch on rails so that downfall is avoided. Closely related but not the same is in the case of computer systems, where analogous property is called fault-tolerance."]}),(0,i.jsxs)("p",{className:"pt-5",children:["In computer systems you have several modules interconnected with varying degree of importance to overall system. You say a system is ",(0,i.jsx)("b",{children:"fault tolerant"}),"  when it is simply able to continue operating despite one (or many) failing component. I like the analogy to the ship sailing through the sea. If it's trunk is just a single empty vessel then hole in the surface will make water to pour in resulting in sinking the ship. If however trunk is separated into several isolated compartments, surface damage will affect only slight part of the whole body."]}),(0,i.jsxs)("p",{className:"pt-5",children:["Complementing fix to isolating failed component is ",(0,i.jsx)("b",{children:"switching to other replica on failure"})," or failover in short. If we know that service is susceptible to failing let's have ",(0,i.jsx)("b",{children:"many instances of it"})," in parallel. Then the traffic is routed only to healthy ones. This duplication can be achieved in two distinct ways: either through ",(0,i.jsx)("b",{children:"redundancy"})," or ",(0,i.jsx)("b",{children:"replication"}),". Sometimes they are used interchangably in literature, but there is subtle difference between them."]}),(0,i.jsxs)("p",{className:"pt-5",children:["A technique of having multiple identical services working in parallel is called redundancy. Services know nothing about each other and if they are ",(0,i.jsx)("b",{children:"stateful"})," their state is not synchronized in any way (related: ",(0,i.jsx)("b",{children:"database sharding"}),"). In front of these services there is an entity called ",(0,i.jsx)("b",{children:"load balancer"}),". It is aware which services are active by constantly pinging them. And those that are (",(0,i.jsx)("b",{children:"healthy"}),"), get some part of the traffic. Now again, load balancer becomes single point of failure. However chances of it failing are much lower because it is only pass-through component. Act of directing traffic to specific service is called ",(0,i.jsx)("b",{children:"routing"})," and any pattern is applicable, as long as it distributes incoming requests evenly across microservices. Most commonly it is ",(0,i.jsx)("b",{children:"round-robin"})," fashion in which each service gets it's turn."]}),(0,i.jsxs)("p",{className:"pt-5",children:["Replication to some degree resembles redundancy, but there is one crucial difference. Instances do have knowledge about each other and collectively make the decisions regarding their state updates. You can see this technique in databases and key-value stores. This makes a ",(0,i.jsx)("b",{children:"cluster"})," considerably slower but each ",(0,i.jsx)("b",{children:"replica"})," has exactly same state. Most notable examples are Redis, etcd and any replicated database."]}),(0,i.jsx)("h2",{className:"text-2xl pt-14 py-8 font-bold",children:"About CAP theorem"}),(0,i.jsxs)("p",{children:[(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://www.researchgate.net/figure/Visualization-of-CAP-theorem_fig2_282679529",children:(0,i.jsx)("b",{children:"CAP theorem"})})," is most widely known theorem applying to distributed systems. It was introduced by ",(0,i.jsx)("b",{children:"Eric Brewer"})," over 20 years ago. You learn from it that there is a fundamental tradeoff between ",(0,i.jsx)("b",{children:"partition tolarance"}),", ",(0,i.jsx)("b",{children:"availability"})," and ",(0,i.jsx)("b",{children:"consistency"}),". But it sounds a little bit too abstract right now, let's motivate contents of this chapter with some example..."]}),(0,i.jsx)("p",{className:"pt-5",children:"Imagine that somewhere in the world there is an online shop. To make the website more interactive company decided to display on top of the page what products are being bought most recently (in realtime). Consequently in the header there should be a dynamically changing label that shows an item name with a link to add it to cart."}),(0,i.jsx)("p",{className:"pt-5",children:"In the first iteration developers enclosed a logic in a single service. It's responsibilities were: maintaining (CRUD) of most recent item and persisting items' stream in the log for Data Analysts. The service was deployed to single compute instance in the cloud. Some time has passed and they faced following problems:"}),(0,i.jsxs)("ul",{className:"px-14 py-4 list-disc",children:[(0,i.jsx)("li",{children:"there were 10 times more reads than writes and compute instance although scaled vertically several times stopped to keep up with read requests"}),(0,i.jsxs)("li",{children:["spikes in traffic brought down service for several hours couple of times, which made adjacent services to also fail - ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://avinetworks.com/glossary/single-point-of-failure/",children:(0,i.jsx)("b",{children:"single point of failure"})})]}),(0,i.jsxs)("li",{children:["during downtime items' stream was not populated to log and this made it unreliable for analytics - ",(0,i.jsx)("b",{children:"no replication"})]})]}),(0,i.jsxs)("p",{className:"pt-3",children:["You can solve the problem of scaling by adding redundancy. Serval service instances can be put behind load balancer as mentioned previously. But they are stateful and this means that each service will have a separate view of state. This would be fine, but there is also requirement to have consistent items' log so ",(0,i.jsx)("b",{children:"replication"})," is really what you are really after. Developers came up with following cluster architecture for service:"]}),(0,i.jsxs)("ul",{className:"px-14 py-2 list-disc",children:[(0,i.jsx)("li",{children:"Assume there are currently 3 nodes."}),(0,i.jsx)("li",{children:"One node is elected a leader and does both reads and writes. In case of write it replicates it to other 2 followers"}),(0,i.jsx)("li",{children:"But the reads are spread evenly across the nodes"})]}),(0,i.jsx)("p",{className:"pt-5",children:"There is a tough choice ahead, however. It concerns how this replication is conducted. To update current item on leader node you  also have to let followers know of it. And suppose that network connection to one of the followers fails during the update. You can release interested client right away with OK message not blocking on replication, but the nodes in cluster will have different state for some time, until update successfully passes through the node on some retry. On the other hand if you block on having successfull update to all follower nodes, you need to put client on hold for (a little) longer time."}),(0,i.jsxs)("figure",{className:"md:px-40 py-25 mt-10",children:[(0,i.jsx)("img",{alt:"CAP Theorem Illustrated",src:"/drawing-4.svg",width:"80%",height:"80%"}),(0,i.jsx)("figcaption",{className:"font-bold text-center",children:"CAP theorem illustrated"})]}),(0,i.jsxs)("p",{className:"pt-5",children:["That's exactly what you learn in ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://www.ibm.com/cloud/learn/cap-theorem",children:(0,i.jsx)("b",{children:"CAP theorem"})}),". In face of ",(0,i.jsx)("b",{children:"network partitions"})," it is only possible to choose between ",(0,i.jsx)("b",{children:"availablity"})," and ",(0,i.jsx)("b",{children:"consistency"}),". You can't have both at the same time. Waiting for information to propagate to all nodes freezes the sender and you lose availablity. Otherwise, by not holding back node state becomes inconsistent among the nodes - a ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://en.wikipedia.org/wiki/Split-brain_(computing)",children:(0,i.jsx)("b",{children:"split brain"})})," happens. Theorem conditions this tradeoff on network failure, but with all connections working we still have to choose beween ",(0,i.jsx)("b",{children:"latency"})," and ",(0,i.jsx)("b",{children:"consistency"}),". If you are interested read ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"http://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf",children:(0,i.jsx)("b",{children:"PACELC"})}),"."]}),(0,i.jsxs)("p",{className:"pt-5",children:["Current trend in industry is to pick availability. Any inconsistencies are fixed later by some background synchronization job. This approach is known as ",(0,i.jsx)("b",{children:"eventual consistency"}),". CAP theorem says about all or nothing situation - either availability or consistency to choose from. However, the problem is more like a scale spreading from availability on left end and consistency on right end. There are many distributed system protocols that place themselves differently on this scale, but eventual consistency is roughly in the middle."]}),(0,i.jsx)("h2",{className:"text-2xl pt-14 py-8 font-bold",children:"Paxos theorem and proof"}),(0,i.jsxs)("p",{children:[(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://lamport.azurewebsites.net/pubs/lamport-paxos.pdf",children:(0,i.jsx)("b",{children:"In 1998 in ACM Transactions on Computer Systems"})})," Leslie Lamport presented a protocol to reach ",(0,i.jsx)("b",{children:"consensus"})," (or consistency) in distributed systems of nodes that can fail. Later this gave him a ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://amturing.acm.org/award_winners/lamport_1205376.cfm",children:(0,i.jsx)("b",{children:"Turing Award"})}),". Although presented in very entertaining way, protocol and theorem were described with mathematical rigor and formality. Understanding paper contents happened to be very challenging for most of the readers, especially students. Two years later, Lamport supplemented his work with ",(0,i.jsx)("a",{className:"hover:text-yellow-500",href:"https://lamport.azurewebsites.net/pubs/paxos-simple.pdf",children:(0,i.jsx)("b",{children:"another, simpler writeup"})})," to make Paxos more approachable to wider audience."]}),(0,i.jsxs)("p",{className:"pt-5",children:["Why bother? It is a backbone of so called ",(0,i.jsx)("b",{children:"consitent core"})," distributed systems pattern. It helps to maintain some state for other services with no single point of failure and no downtime. Paxos helps to achieve consistency and tolerates failures up to N/2-1 nodes."]}),(0,i.jsxs)("p",{className:"pt-5",children:["The theorem is cast in a following context: you have two kinds of nodes, a single leader and followers. The leaders job is to receive state updates and orchestrate communication in such a way that all the nodes agree on this new value. It does this in series of ",(0,i.jsx)("b",{children:"ballots"}),". For each ballot leader chooses some random subset of nodes called ",(0,i.jsx)("b",{children:"quorum"}),". Each ballot has it's unique monotonically increasing number. In one ballot state's ",(0,i.jsx)("b",{children:"version"})," (I changed terminology a bit) is proposed to quorum and they acknowledge it with a message back to the leader. Here is the catch however: these nodes can fail and not respond. Nodes that responded are called ",(0,i.jsx)("b",{children:"voting set"}),". So voting set is a subset of the quorum."]}),(0,i.jsx)("h2",{className:"font-bold py-8",children:"Paxos Theorem"}),(0,i.jsx)("p",{children:(0,i.jsx)("i",{children:"Given following three conditions:"})}),(0,i.jsxs)("ul",{className:"px-14 py-2 list-disc italic",children:[(0,i.jsx)("li",{children:"C1: each ballot has unique number"}),(0,i.jsx)("li",{children:"C2: between any pair of quorums (of ballots) there is at least one node in common"}),(0,i.jsx)("li",{children:"C3: during ballot president polls each quorum's node the version they voted for recently. Then the most recent one is always chosen from this set."})]}),(0,i.jsx)("p",{className:"pt-2",children:(0,i.jsx)("i",{children:"If any ballot's quorum size equals voting set's size (voting passed) then no subsequent ballot will change that (passed) version."})}),(0,i.jsx)("h2",{className:"font-bold py-8",children:"Proof by contadiction"}),(0,i.jsx)("p",{children:"Let's say there are two ballots one after another: B0 and B1. In ballot B0 voting passed and elected version V0. Now assume by contradiction that ballot B1 also passed and elected version V1 != V0. But from condition C2 we know that there is at least one node in common between B0 and B1. This nodes version was V0, since it was most recent by C3 there is no other way but to choose V0 in ballot B1. In fact we just proved an induction step that applied proves that the same holds for any ballot B2, B3, ..., BN."}),(0,i.jsx)("h2",{className:"text-2xl pt-14 py-8 font-bold",children:"Paxos protocol"}),(0,i.jsx)("p",{className:"",children:"And from this proved theorem Lamport derived protocol as follows:"}),(0,i.jsxs)("ul",{className:"px-14 py-2 list-decimal",children:[(0,i.jsxs)("li",{children:["In the beginning (and whenever old leader is down) leader has to be elected. Beacuse each node has it's own id we can choose whatever convention that fits. For example let's pick the one that smallest id designates a leader. Notion of ",(0,i.jsx)("b",{children:"timeouts"})," help to know that no nodes is leaving during this phase. Let's assume that during election no node leaves the cluster. Then each node broadcasts it's own id and waits until some time has passed. If it received N-1 ids from other nodes and his id is smallest it elects himself a leader and starts subsequent voting. Otherwise he places himself in position of ordinary node."]}),(0,i.jsxs)("li",{className:"pt-2",children:["If you remember there are two pieces of infromation on each node. ",(0,i.jsx)("b",{children:"Last vote"})," is the latest state version node has been voting on and this one is not (and can't be consistent). On the other hand ",(0,i.jsx)("b",{children:"ledger"}),' is a log of consistent "state snapshots" which means this information is committed to collective memory and ultimately will be on every node. What is more versions in ledger are ordered and strictly increasing order relative to the order of voting rounds. Whenever new version arrives at any of the nodes he forwards it to a leader to start first round of voting. During that time no one has voted yet so this version is trivially passed (if all quorum members acknowledge it).']}),(0,i.jsxs)("li",{className:"pt-2",children:["In subsequent rounds in this step, the leader want's to know ",(0,i.jsx)("b",{children:"last voted"})," version of each of the nodes in quorum (he picks the quorum at random with n/2+1 nodes). All nodes need to respond with this piece of information otherwise new voting round has to be started (with different quorum)"]}),(0,i.jsxs)("li",{className:"pt-2",children:["Leader picks most recent ",(0,i.jsx)("b",{children:"last voted"})," version from all of the nodes and wants all the quorum nodes to accept it. He sends them a message stating a version it has picked. Nodes have to respond with ACK message. Again if some of nodes don't respond new voting round has to be started. Otherwise leader considers this version ",(0,i.jsx)("b",{children:"commited"})," (",(0,i.jsx)("b",{children:"voting passed"}),") and writes it to his ledger. Now it broadcasts that voting has passed and quorum nodes can put this version to their ledgers also."]}),(0,i.jsx)("li",{className:"pt-2",children:"Quorum members fill the version in their ledgers. "})]}),(0,i.jsx)("p",{className:"pt-5",children:"This step by step description sums up Paxos protocol and consistency in general. I intentionally tried to keep it short and basic if you are interested in details and corner-cases then refer to the linked paper."})]})]})})}},9008:function(e,t,s){e.exports=s(5443)}},function(e){e.O(0,[774,888,179],(function(){return t=7526,e(e.s=t);var t}));var t=e.O();_N_E=t}]);