(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[545],{9526:function(e,n,o){(window.__NEXT_P=window.__NEXT_P||[]).push(["/posts/unmanaged-kubernetes-clusters",function(){return o(3807)}])},3807:function(e,n,o){"use strict";o.r(n);var t=o(5893),r=o(9008);n.default=function(){return(0,t.jsx)("div",{children:(0,t.jsxs)("article",{className:"md:w-3/5 text-lg mr-auto ml-auto",children:[(0,t.jsxs)(r.default,{children:[(0,t.jsx)("link",{rel:"stylesheet",href:"/css/highlight-2.css"}),(0,t.jsx)("script",{type:"text/javascript",src:"/js/terraform.js"}),(0,t.jsx)("script",{defer:!0,children:"hljs.registerLanguage('terraform', window.hljsDefineTerraform); hljs.highlightAll();"}),(0,t.jsx)("title",{children:"k8s on GCP: a walkthrough"}),(0,t.jsx)("meta",{name:"Description",content:"Starting with benefits of managing kuberentes cluster, I explain how to setup one."})]}),(0,t.jsxs)("div",{className:"p-14 flex flex-col",children:[(0,t.jsx)("h1",{className:"text-3xl font-bold text-center",children:"k8s on GCP: a walkthrough"}),(0,t.jsx)("p",{children:"Kubernetes has been mainstream for some time already. And everybody seems to have already moved to cloud. At work I had a bit of GCP and that time found devops work pretty cool. This post is a challenge that I went through: setting up Kubernetes on bare VMs."}),(0,t.jsx)("p",{children:"It is like a tutorial so you have to read it from top to bottom. And configured k8s cluster is a result. It touches on a lot of things regarding cluster administration and GCP in general. "}),(0,t.jsx)("p",{children:"But maybe before we dive straight in let's see if this work would be useful at all. Below is pros and cons of setting up cluster by yourself versus cluster that is provisioned for you by GCP."}),(0,t.jsxs)("table",{className:"m-10 my-3 table-fixed mt-5 border border-black object-center",children:[(0,t.jsx)("thead",{children:(0,t.jsxs)("tr",{children:[(0,t.jsx)("th",{className:"w-1/3 bg-black text-white border border-black text-center",children:(0,t.jsx)("b",{children:"Managed"})}),(0,t.jsx)("th",{className:"w-1/3 bg-black text-white border border-black text-center",children:(0,t.jsx)("b",{children:"Unmanaged"})})]})}),(0,t.jsxs)("tbody",{children:[(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Control plane is fully managed and configuration is impossible or serverly limited"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Full control of control plane configuration any administrative actions can be taken"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Number of worker nodes scales automatically with increase in load"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Manual administrative work is needed to scale"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Nodes limited to running on OS images given by cloud provider"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Possiblity to bake your own OS images"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Nodes run on VMs which state is completely hidden from devops"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Full control over nodes' VMs"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Integrated logging and monitoring"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Manual setup of logging and monitoring"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Filesystem not persistent over restarts"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Possiblity to backup and setup persistent storage"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Automatic upgrades and autohealing"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Manual maintanance of individual nodes or custom automation"})]}),(0,t.jsxs)("tr",{children:[(0,t.jsx)("td",{className:"border border-black text-center",children:"Authorization and Authentication by Google IAM"}),(0,t.jsx)("td",{className:"border border-black text-center",children:"Authorization and Authentication by bring your own certificates, passwords, proxies"})]})]})]}),(0,t.jsx)("h2",{className:"text-2xl pt-14 font-bold",children:"Architecture design"}),(0,t.jsxs)("p",{children:["Before laying out the components it's good to know what purpose our cluster will serve. These are the ",(0,t.jsx)("b",{children:"constraints"}),":"]}),(0,t.jsxs)("ul",{className:"px-5 py-2 list-disc",children:[(0,t.jsx)("li",{children:"if one physical location fails this shouldn't affect both control plane and workers"}),(0,t.jsx)("li",{children:"workloads are small and only for demonstration purposes"}),(0,t.jsx)("li",{children:"setup should be maximally simple"})]}),(0,t.jsxs)("p",{children:["There are ",(0,t.jsx)("a",{href:"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/",children:(0,t.jsx)("b",{children:"2 approaches"})})," to setup control plane: one colocates etcd instances with other Kubernetes components (easier), other externalizes etcd to separate cluster (harder). Google Cloud Platform for most of the resources offers at most regional resiliency. Zone are servers in one building in some city of your choosing and region is couple of such buildings in this city. Global resiliency is achieved by having clusters of resources duplicated on each continent, with switching clients to the closest cluster available. It's because latencies across continents are simply to big to have cluster distributed around the world."]}),(0,t.jsxs)("p",{children:[" Let's have three control plane nodes in regional configuration: each in separate zone. And workloads are small so 3 workers is more than enough. This way ",(0,t.jsx)("b",{children:"high availability"})," will be achieved at minimal cost. Control plane will be side by side with etcd to omit separate etcd cluster setup. Like in this diagram:"]}),(0,t.jsxs)("figure",{className:"md:px-20 py-8",children:[(0,t.jsx)("img",{alt:"High availability cluster",src:"/drawing-3.svg",width:"90%",height:"90%"}),(0,t.jsx)("figcaption",{className:"font-bold text-center pt-5",children:"High availability cluster"})]}),(0,t.jsx)("h2",{className:"text-2xl pt-14 py-8 font-bold",children:"Setup and tools"}),(0,t.jsxs)("p",{children:["This is a lot of work and some time ago it had to be done manually. These days however ",(0,t.jsx)("a",{href:"https://en.wikipedia.org/wiki/Infrastructure_as_code",children:(0,t.jsx)("b",{children:"infrastracture as code"})})," is taking over the world. You maintain declarative definition and automation tools handle creation, deletion and updates of needed resources. By now, the most popular tool is ",(0,t.jsx)("a",{href:"https://www.terraform.io/",children:(0,t.jsx)("b",{children:"Terraform"})})," and I used it in this post."]}),(0,t.jsx)("p",{children:"To start coding Terraform scripts you need to setup couple of GCP related things:"}),(0,t.jsxs)("ul",{className:"px-7 list-disc",children:[(0,t.jsx)("li",{children:"create GCP project and enable billing"}),(0,t.jsx)("li",{children:"enable Compute API from the Console"}),(0,t.jsx)("li",{children:"download and install gcloud SDK"}),(0,t.jsx)("li",{children:"create service account with editor permissions and download the key"}),(0,t.jsx)("li",{children:"install Terraform"})]}),(0,t.jsx)("h2",{className:"text-2xl pt-14 font-bold",children:"Terraform providers and utilities"}),(0,t.jsxs)("p",{className:"",children:[" Terraform structures project as a ",(0,t.jsx)("a",{href:"https://www.terraform.io/docs/language/modules/develop/structure.html",children:(0,t.jsx)("b",{children:"folder"})})," with script and data files. All of Terraform code goes to ",(0,t.jsx)("i",{children:"main.tf"})," file. You can extract variables to ",(0,t.jsx)("i",{children:"variables.tf"})," file, but I won't use that feature as the script will be really simple. In script file you define resources which you want to setup (not only) on the cloud. Running the script triggers build of ",(0,t.jsx)("b",{children:"Resource Graph"}),": dependencies are ordered and non-dependant resources are free to be provisioned in parallel. Before actual actions happen Terraform displays ",(0,t.jsx)("b",{children:"Execution Plan"})," that lists all the operations to be done. Steps already finished and resource modifications are persisted in special file ",(0,t.jsx)("i",{children:"tfstate"})," so that only incremental changes are applied. Once you are happy with the script you run ",(0,t.jsx)("code",{children:"terraform apply"})," to provision infrastructure or ",(0,t.jsx)("code",{children:"terraform destroy"})," to tear it down. This is the first part of ",(0,t.jsx)("code",{children:"main.tf"}),":"]}),(0,t.jsx)("pre",{className:"mt-3 p-5",children:(0,t.jsx)("code",{className:"language-terraform",children:'\nterraform {                                          // 1\n  required_providers {\n    google = {\n      source = "hashicorp/google"\n      version = "3.5.0"\n    }\n  }\n}\n\nprovider "google" {                                 // 2\n  credentials = file("service-account-key.json")\n\n  project = "dg-kubernetes-2"\n  region  = "europe-west3"\n  zone    = "europe-west3-a"\n}\n\nprovider "google-beta" {                            // 3\n  credentials = file("service-account-key.json")\n\n  project = "dg-kubernetes-2"\n  region  = "europe-west3"\n  zone    = "europe-west3-a"\n}\n\nlocals {                                            // 4\n  control_ips = ["10.1.0.10", "10.1.0.11", "10.1.0.12"]\n  worker_ips = ["10.1.0.13", "10.1.0.14", "10.1.0.15"]\n  control_zones = ["europe-west3-a","europe-west3-b","europe-west3-c"]\n  worker_zones = ["europe-west3-a","europe-west3-b","europe-west3-c"]\n}\n'})}),(0,t.jsxs)("ul",{className:"px-8 list-decimal",children:[(0,t.jsxs)("li",{children:["Terraform interacts with the cloud and servers by ",(0,t.jsx)("a",{href:"https://www.terraform.io/docs/language/providers/index.html",children:(0,t.jsx)("b",{children:"plugins called providers"})}),". This block shows which providers are required in the script."]}),(0,t.jsx)("li",{children:"This is the configuration of provider. Similar to gcloud SDK, plugin requires project, region and zone labels. Next, GCP Service Account's JSON key (put in our project directory) contents are read."}),(0,t.jsx)("li",{children:"GCP provider has couple of resources in beta stage and they reside in separate plugin. It is configured the same way."}),(0,t.jsx)("li",{children:"Terraform allows to keep local variables in one place."})]}),(0,t.jsx)("h2",{className:"text-2xl pt-14 font-bold",children:"Configuring VPC network"}),(0,t.jsxs)("p",{children:[" Kubernetes cluster requires ",(0,t.jsx)("a",{href:"https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html",children:(0,t.jsx)("b",{children:"two network interfaces"})}),". First one serves communication on control plane, but second is for virtual, inside-kubernetes network and assigns each pod unique IP only inside cluster. We can configure it in two ways. An ",(0,t.jsx)("a",{href:"https://cloud.google.com/vpc/docs/alias-ip",children:(0,t.jsx)("b",{children:"Alias IP"})})," is a secondary interface for each VM that fits this use case nicely. Or second option is to enable VMs to forward misrouted packets to each other. Google Cloud Platform's VPC Network is a global resource. It is built from Subnets that are scoped to region. They are basically IP ranges from which you can assign individual IPs to Virtual Machines (inside the given region). Yet thats not everything - you also have to enable receiving or sending traffic by security rules either by Routes or Firewall Rules.  ",(0,t.jsx)("a",{href:"https://cloud.google.com/vpc/docs/routes",children:(0,t.jsx)("b",{children:"Routes"})})," deal with outbound traffic from VMs. ",(0,t.jsx)("a",{href:"https://cloud.google.com/vpc/docs/firewalls",children:(0,t.jsx)("b",{children:"Firewall Rules"})})," enable/disable inter-VM traffic."]}),(0,t.jsxs)("p",{children:["We configure VMs to have IPs from single subnet. Next inter-VM traffic by TCP, UDP and ICMP is enabled. Both ranges primary and inside kubernetes are taken into account. Kubernetes ",(0,t.jsx)("b",{children:"API Server"})," talks to ",(0,t.jsx)("b",{children:"kubectl"})," on TCP 6443, so traffic from default gateway (0.0.0.0) on this port has also to be enabled. This gives following piece of code:"]}),(0,t.jsx)("pre",{className:"mt-3 p-5",children:(0,t.jsx)("code",{className:"language-terraform",children:'\nresource "google_compute_network" "vpc_network" {         \n  name = "kubernetes-network"\n}\n\nresource "google_compute_subnetwork" "vpc_subnet" {       \n  name          = "kubernetes-subnet"\n  ip_cidr_range = "10.1.0.0/24"\n  region        = "europe-west3"\n  network       = google_compute_network.vpc_network.id\n}\n\nresource "google_compute_firewall" "fr_internal" {        // 1\n  name    = "kubernetes-fr-internal"\n  network = google_compute_network.vpc_network.name\n\n  allow {\n    protocol = "icmp"\n  }\n\n  allow {\n      protocol = "tcp"\n  }\n\n  allow {\n      protocol = "udp"\n  }\n\n  \n  source_ranges = [ "10.1.0.0/24", "10.2.0.0/16" ]\n}\n\nresource "google_compute_firewall" "fr_external" {        // 2\n  name    = "kubernetes-fr-external"\n  network = google_compute_network.vpc_network.name\n\n\n  allow {\n    protocol = "icmp"\n  }\n\n  allow {\n      protocol = "tcp"\n      ports = [ "22" , "6443" ]\n  }\n\n  source_ranges = ["0.0.0.0/0"]\n}\n\nresource "google_compute_firewall" "fr_external2" {       // 3\n  name    = "kubernetes-fr-external2"\n  network = google_compute_network.vpc_network.name\n  direction = "EGRESS"\n\n  allow {\n      protocol = "tcp"\n      ports = ["6443" ]\n  }\n\n  destination_ranges = ["0.0.0.0/0"]\n}\n'})}),(0,t.jsxs)("ul",{className:"px-14 list-decimal",children:[(0,t.jsx)("li",{children:"This part enables inter-VM communication. "}),(0,t.jsx)("li",{children:"We have to enable HTTPS for kubectl and SSH to log into machine."}),(0,t.jsx)("li",{children:"Required for health checks, more on this later."})]}),(0,t.jsx)("h2",{className:"text-2xl pt-14 font-bold",children:"Configuring Virtual Machines on GCP"}),(0,t.jsxs)("p",{children:["The essence of the cluster are of course Virtual Machines. They are virtualized computing resources with configurable number of vCPUs (machine type). They are scoped to single zones. Any VM has primary hard drive called ",(0,t.jsx)("b",{children:"boot disk"})," on which there is operating system present. Each cloud provider maintains range of OS images, both Windows and Linux from which you can init this disk. Each VM has also IP on its primary network interface."]}),(0,t.jsxs)("p",{children:["Continuing configuration  minimal number resources used by VMs is set up. Similar, but not the same are definitions of control plane and worker instances. Kubernetes has to be installed on each machine separately. To save yourself some work there is possilibity to run common script on init of machine by ",(0,t.jsx)("b",{children:"metascript"}),"."]}),(0,t.jsx)("pre",{className:"mt-3 p-5",children:(0,t.jsx)("code",{className:"language-terraform",children:'\nresource "google_compute_instance" "control_plane_instances" {        // 1\n  name         = "control-plane-instance${count.index}"\n  machine_type = "e2-standard-2"\n  zone = local.control_zones[count.index]\n  count = 3                                                           // 2\n\n  boot_disk {                                                         // 3\n    initialize_params {\n      image = "ubuntu-os-cloud/ubuntu-1804-lts"\n      size = 128\n    }\n  }\n\n  network_interface {                                                 // 4\n    network = google_compute_network.vpc_network.name\n    subnetwork = google_compute_subnetwork.vpc_subnet.name\n    network_ip = local.control_ips[count.index]\n    access_config {\n    }\n  }\n\n  can_ip_forward = true                                               // 5\n\n  metadata_startup_script = "${file("init.sh")}"                      // 6\n}\n\nresource "google_compute_instance" "worker_instances" {               // 7\n  name         = "worker-instance${count.index}"\n  machine_type = "e2-standard-2"\n  zone = local.worker_zones[count.index]\n  count = 3\n\n  boot_disk {\n    initialize_params {\n      image = "ubuntu-os-cloud/ubuntu-1804-lts"\n      size = 128\n    }\n  }\n\n  network_interface {\n    network = google_compute_network.vpc_network.name\n    subnetwork = google_compute_subnetwork.vpc_subnet.name\n    network_ip = local.worker_ips[count.index]\n    access_config {\n    }\n  }\n\n  can_ip_forward = true\n\n  metadata_startup_script = "${file("init.sh")}"\n}\n}\n'})}),(0,t.jsxs)("ul",{className:"px-14 list-decimal",children:[(0,t.jsx)("li",{children:"The block provides configuration for control plane instances."}),(0,t.jsxs)("li",{children:[(0,t.jsx)("code",{children:"count"})," is a special attribute in Terraform - it triggers a loop in which enclosing block is repeated. Iterated values are substituted by ",(0,t.jsx)("code",{children:"count.index"}),". Recall that we declared array of IPs in subnet range for control plane. Now they are filled in network section."]}),(0,t.jsx)("li",{children:"Boot disk initial parameters."}),(0,t.jsx)("li",{children:"VMs connect to network and subnet created earlier."}),(0,t.jsx)("li",{children:"Very important, it allows VMs to forward packets to other VMs, allowing Kuberenetes nested network to function."}),(0,t.jsx)("li",{children:"Pre-provisioning script to run on VMs boot."}),(0,t.jsx)("li",{children:"Similar configuration is for worker nodes with couple of exceptions."})]}),(0,t.jsxs)("p",{children:["Since OS is linux distribution, provisioning script is just bash script. The contents of ",(0,t.jsx)("code",{children:"init.sh"})," are:"]}),(0,t.jsx)("pre",{className:"mt-3 p-5",children:(0,t.jsx)("code",{className:"language-shell",children:'\nsudo apt-get update\n\nsudo apt-get install -y \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg \\\n    lsb-release\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | \\\nsudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\ncurl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg \\\nhttps://packages.cloud.google.com/apt/doc/apt-key.gpg\n\necho   "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg]  https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n\necho "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list\n\nsudo apt-get update\n\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io kubelet kubeadm kubectl\n\nsudo apt-mark hold kubelet kubeadm kubectl\n'})}),(0,t.jsx)("p",{children:"This script installs docker (running containers), kubelets (network proxy for Kubernetes) and kubeadm (administration tool)."}),(0,t.jsx)("h2",{className:"text-2xl pt-14 font-bold",children:"GCP load balancer"}),(0,t.jsxs)("p",{children:["In this section frontend for control plane is configured. If you remember there are three control plane nodes listening on port 6443 for connection from kubectl. Google Cloud Platform provides load balancer, but to connect it to VMs they have to be wrapped in ",(0,t.jsx)("b",{children:"Instance Groups"}),". Moreover ",(0,t.jsx)("b",{children:"Health Check"})," needs to be configured to constantly ping those VMs. On failed pings from VM, load balancer will redirect traffic to healthy ones. Kubernetes operates in TCP/IP layer, so external TCP load balancer has to be configured (as opposed to HTTP/HTTPS load balancer)."]}),(0,t.jsx)("pre",{className:"mt-3 p-5",children:(0,t.jsx)("code",{className:"language-terraform",children:'\nresource "google_compute_instance_group" "control_plane_igs" {                          // 1\n  name        = "control-plane-instance-group${count.index}"\n  count = 3\n  zone = local.control_zones[count.index]\n\n  named_port {\n    port = 6443\n    name = "control"\n  }\n \n  instances = [google_compute_instance.control_plane_instances[count.index].self_link]\n}\n\nresource "google_compute_region_backend_service" "control_plane_backend" {            // 2\n  region = "europe-west3"\n  name      = "control-plane-backend"\n  protocol  = "TCP"\n  provider = google-beta\n\n  backend {\n    group = google_compute_instance_group.control_plane_igs[0].id\n  }\n  \n  backend {\n    group = google_compute_instance_group.control_plane_igs[1].id\n    \n  }\n\n   backend {\n    group = google_compute_instance_group.control_plane_igs[2].id\n    \n  }\n\n  port_name = "control"\n\n  health_checks = [\n    google_compute_region_health_check.tcp_health_check.id\n  ]\n\n  load_balancing_scheme = "EXTERNAL"\n}\n\nresource "google_compute_region_health_check" "tcp_health_check" {                // 3\n  name     = "tcp-region-health-check"\n\n  timeout_sec        = 1\n  check_interval_sec = 1\n\n  tcp_health_check {\n    port = "6443"\n  }\n}\n\nresource "google_compute_forwarding_rule" "default" {                             // 4\n  name                  = "control-plane-forwarding-rule"\n  region                = "europe-west3"\n  port_range            = 6443\n  backend_service       = google_compute_region_backend_service.control_plane_backend.id\n  \n  load_balancing_scheme = "EXTERNAL"\n  provider = google-beta\n}\n\n'})}),(0,t.jsxs)("ul",{className:"px-14 list-decimal",children:[(0,t.jsx)("li",{children:"To connect VMs to backend instance groups need to be created. Here each has only one VM."}),(0,t.jsxs)("li",{children:["Load balancer connects traffic to ",(0,t.jsx)("b",{children:"backends"}),", each of them points to Instance Group. "]}),(0,t.jsx)("li",{children:"Health check."}),(0,t.jsx)("li",{children:"Finally load balancer is defined."})]}),(0,t.jsxs)("p",{children:["At this point you run ",(0,t.jsx)("code",{children:"terraform apply"})," and wait until completion. After completion write down load balancer's external address from ",(0,t.jsx)("code",{children:"terraform.tfstate"})]}),(0,t.jsx)("h2",{className:"text-2xl pt-14 font-bold",children:"Provisioning with kubeadm"}),(0,t.jsxs)("p",{children:["Now the infrastructure is ready on Google Cloud Platform. Some manual work is required to form a cluster, install pod network and setup connection for admin. ",(0,t.jsx)("b",{children:"Important! Two control plane nodes must be disabled temporarily to not confuse load balancer while setting up control plane. This can be done from the GCP console."}),"  Lets log in by SSH into on VM."]}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\ngcloud config set compute/region europe-west3\ngcloud config set compute/zone europe-west3-a\ngcloud compute ssh control-plane-instance0\n"})}),(0,t.jsx)("p",{children:"...and init kubernetes control plane node:"}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\nsudo kubeadm init --control-plane-endpoint=<lb ip>:6443 \\\n  --pod-network-cidr=10.2.0.0/16 --upload-certs\n"})}),(0,t.jsx)("p",{children:"It begs for some explanation: we allowed second CIDR range in firewall. Now it is provided to kubeadm to form cluster connectivity. Then external IP for cluster needs to be specified (or DNS entry). Upload certs flag makes sure new control plane nodes receive certificate. This certificate is signed for all control plane nodes and load balancer's IP. The long output provides command we have to run on other two nodes."}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:'\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun "kubectl apply -f [podnetwork].yaml" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of the control-plane node running \nthe following command on each as root:\n\n  kubeadm join 34.107.71.115:6443 --token 66mohe.71so638ousw8lh96 \\\n        --discovery-token-ca-cert-hash \\\n        sha256:c8292bb6bc99cc151d11a0c5c7d463b579990098f0d4d41f0263610f7f92f07f \\\n        --control-plane --certificate-key \\\n        4a6a19f2f3fac9846f9a5b20e7813640ad628739e68c09f67f1dd6733563c129\n\nPlease note that the certificate-key gives access to cluster sensitive data, keep it secret!\nAs a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use\n"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 34.107.71.115:6443 --token 66mohe.71so638ousw8lh96 \\\n        --discovery-token-ca-cert-hash \\\n        sha256:c8292bb6bc99cc151d11a0c5c7d463b579990098f0d4d41f0263610f7f92f07f\n'})}),(0,t.jsxs)("p",{children:["We continue an join other 2 nodes. Lastly we join 3 worker nodes by other command from the above output. Then pod network needs to be installed, by downloading ",(0,t.jsx)("code",{children:"Calico"})," (or any other) pod network controller."]}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\ncurl https://docs.projectcalico.org/manifests/calico.yaml -O\n"})}),(0,t.jsxs)("p",{children:["In given YAML file default ",(0,t.jsx)("code",{children:"196.168.0.0/16"})," CIDR range has to be substituted to our secondary range. Then we are free to apply operator in kubernetes: ",(0,t.jsx)("code",{children:"kubectl apply -f calico.yaml"}),". When we list the nodes  (",(0,t.jsx)("code",{children:"kubectl get nodes"}),") there should be status visible that they are ready."]}),(0,t.jsx)("p",{children:"The last step is providing admin an access to the control plane. He will use his custom made certificate. First we create Certificate Signing Request on local computer:"}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:'\nmkdir keys\nopenssl genrsa -out keys/admin.key 2048\nopenssl req -new -key keys/admin.key \\\n-out keys/admin.csr -subj "/CN=admin/O=devops"\n'})}),(0,t.jsx)("p",{children:"Next we copy it to first control plane node:"}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\ngcloud compute scp admin.csr control-plane-instance0:~/admin.csr\n"})}),(0,t.jsx)("p",{children:"Next we login to first VM and sign certificate."}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\nopenssl x509 -req \\\n-in ~/admin.csr \\\n-CA /etc/kubernetes/pki/ca.crt \\\n-CAkey/etc/kubernetes/pki/ca.key \\\n-CAcreateserial \\\n-out ~/admin.crt \\\n-days 365\n"})}),(0,t.jsxs)("p",{children:["... and copy back ",(0,t.jsx)("code",{children:"ca.crt"})," and ",(0,t.jsx)("code",{children:"admin.crt"})," to (admin's) computer"]}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\ngcloud compute scp control-plane-instance0:~/dgawlik.crt .\ngcloud compute scp control-plane-instance0:/etc/kubernetes/pki/ca.crt .\n"})}),(0,t.jsx)("p",{children:"The admin (on his machine) has to create new kubeconfig. Remember to fill in load balancer's external IP."}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\nkubectl config set-cluster admin --certificate-authority ca.crt --server <LB IP>\n\nkubectl config set-credentials admin --client-certificate admin.crt --client-key admin.key\n\nkubectl config set-context admin --cluster admin --user admin\n\nkubectl config use-context admin\n"})}),(0,t.jsx)("p",{children:"At this step admin is able to connect to kubernetes without facing network timeouts. He can't do anything in cluster, however, because default kubernetes policy is to deny every action. It's time to fix that. Logging in to first VM again:"}),(0,t.jsx)("pre",{className:"p-5 my-2",children:(0,t.jsx)("code",{className:"language-shell",children:"\nkubectl create rolebinding kube-admin --clusterrole admin --user admin --namespace default\n"})}),(0,t.jsx)("p",{children:"We have given admin pretty serious privileges. At this point cluster is fully operating and admin has access to control plane. This concludes the post."})]})]})})}},9008:function(e,n,o){e.exports=o(5443)}},function(e){e.O(0,[774,888,179],(function(){return n=9526,e(e.s=n);var n}));var n=e.O();_N_E=n}]);