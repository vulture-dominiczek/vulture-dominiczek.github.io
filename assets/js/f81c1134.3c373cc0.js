"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8130],{7735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/03/9/","metadata":{"permalink":"/blog/2025/03/9/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-03-9.md","source":"@site/blog/2025-03-9.md","title":"Semantic search for dynamically built queries in Java and CodeQL","description":"There was a challenge for me recently to search for SQL queries in large codebase. There is a problem","date":"2025-03-09T00:00:00.000Z","tags":[{"inline":true,"label":"codeql","permalink":"/blog/tags/codeql"},{"inline":true,"label":"java","permalink":"/blog/tags/java"},{"inline":true,"label":"sql","permalink":"/blog/tags/sql"}],"readingTime":6.3,"hasTruncateMarker":false,"authors":[],"frontMatter":{"title":"Semantic search for dynamically built queries in Java and CodeQL","tags":["codeql","java","sql"]},"unlisted":false,"nextItem":{"title":"Wring simple parser with Megaparsec in Haskell","permalink":"/blog/2025/03/3/"}},"content":"There was a challenge for me recently to search for SQL queries in large codebase. There is a problem \\nwith using basic grep or even IntelliJ search here because of the performance issues.\\n\\n* queries are long\\n* codebase is large\\n* string searching is not performant enough.\\n\\nAn answer how to solve this task is buried in history of beginnings of static analysis tools. The first tools \\nused basic regexes, but that turned out inefficient pretty quickly. Then incrementally more focus has been \\nput to parse source files to Abstract Syntax Trees which is allows more freedom to write queries. Then \\nfinally Data Flow approach was added alongside Taint Analysis to make current landscape of security today.\\n\\nSemantic searching has 2 advantages:\\n\\n* searching bare tokens is orders of magnitude faster than strings, in turn searching Abstract Syntax Trees \\nis order of magnitude faster than tokens\\n* semantic search offers more precision in designing the queries which only reinforces the first point.\\n\\n**CodeQL** is one such tool that knows the syntax of major languages (Java) and caters for performant search of \\nlarge codebases. I decided to have fun with it over the weekend and push it to it\'s limits as searching for \\ndynamic queries is hard enough. I will show how to set up the project and write some queries for toy source file.\\n\\nLet\'s get started.\\n\\n## CodeQL setup\\n\\nFirst step is downloading [CLI](https://docs.github.com/en/code-security/codeql-cli/getting-started-with-the-codeql-cli/setting-up-the-codeql-cli) and setting it up in a PATH.\\n\\nThen you have to build **database** for the project you will be analyzing. Unfortunately on each code change you have to rebuid it.\\n\\n```\\ncodeql database create --source-root=. --language=java-kotlin --command=\'./mvnw clean compile\' -- tester\\n```\\n\\n`--source-root` - it is used for reporting in queries which give you source locations\\n\\n`--language` - if you want to limit project just to one language, this is an option\\n\\n`--command` - codeQL tracks work of build tool to extract AST and DataFlow information\\n\\nThen in next step download VSCode extension.\\n\\nSelect **QL** on the sidebar.\\n\\nLanugage > set Java.\\n\\nDatabases > From a folder > point to database folder you have built.\\n\\nCtrl+Shift+p > CodeQL: create query\\n\\nAnd you are good to go.\\n\\n## The challenge\\n\\nWe will be operating on Spring Boot project, but of interest is one file only really.\\n\\n```java\\npackage com.example.demo.database;\\n\\nimport java.util.List;\\n\\nimport org.springframework.beans.factory.annotation.Autowired;\\nimport org.springframework.jdbc.core.JdbcTemplate;\\nimport org.springframework.stereotype.Service;\\n\\nimport com.example.demo.valueobject.Purchasing1;\\n\\n@Service\\npublic class WWRepository1 {\\n   \\n   @Autowired\\n   private JdbcTemplate jdbcTemplate;\\n\\n   public List<Purchasing1> getAllPurchasing() {\\n        \\n        StringBuilder aside = new StringBuilder();\\n        aside.append(\\"Hello \\");\\n        aside.append(\\"World\\");\\n        aside.append(\\"!\\");\\n\\n        System.out.println(aside.toString());\\n\\n\\n        StringBuilder sql = new StringBuilder();\\n        sql.append(\\"SELECT Description, SupplierName, BankAccountName, ValidFrom, ValidTo \\");\\n        sql.append(\\"FROM Purchasing.PurchaseOrders o \\");\\n        addJoin1(sql);\\n        addJoin2(sql);\\n\\n        return jdbcTemplate.query(sql.toString(), (rs, rowNum) -> \\n             new Purchasing1(\\n                rs.getString(\\"Description\\"), \\n                rs.getString(\\"SupplierName\\"), \\n                rs.getString(\\"BankAccountName\\"), \\n                rs.getTimestamp(\\"ValidFrom\\").toLocalDateTime(), \\n                rs.getTimestamp(\\"ValidTo\\").toLocalDateTime()\\n            )\\n        );\\n   }\\n\\n   private void addJoin1(StringBuilder sb) {\\n       sb.append(\\"inner join Purchasing.PurchaseOrderLines l on o.PurchaseOrderID = l.PurchaseOrderID \\");\\n   }\\n\\n   private void addJoin2(StringBuilder sb) {\\n      sb.append(\\"inner join Purchasing.Suppliers s on s.SupplierID = o.SupplierID\\");\\n   }\\n}\\n```\\n\\nLet\'s make some assumptions:\\n\\n* codebase is tens of thousands files, but query building is local to single file\\n* queries are built with **StringBuilder** but there can be some noise like unrelated **appends**\\n* query is \\"finalized* with passing it to **jdbcTemplate**\\n* query building can be nested in helper methods which can further nest the logic\\n* query searching has to be case insensitive\\n\\n\\n## Approximate solution\\n\\nMy logic here is the following. Having a file of the given query is good enough, even having false positives. \\nSo let\'s assume that the file is valid if it has all the query chunks. For example WWRepository1.java is valid\\nbecause it has:\\n\\n```\\nSELECT Description, SupplierName, BankAccountName, ValidFrom, ValidTo\\n```\\n\\nor \\n\\n```\\nFROM Purchasing.PurchaseOrders o \\n```\\n\\nor \\n\\n```\\ninner join Purchasing.PurchaseOrderLines l on o.PurchaseOrderID = l.PurchaseOrderID\\n```\\n\\nor \\n\\n```\\ninner join Purchasing.Suppliers s on s.SupplierID = o.SupplierID\\n```\\n\\n\\nA limitation of this approach is that if we have 2 queries in one file, they could intermix and \\ngive a false positive, but let\'s not worry about that for now.\\n\\nLet\'s write first CodeQL query.\\n\\n```\\nfrom \\n    MethodCall e,\\n    Expr part \\nwhere \\n    e.getMethod().hasName(\\"append\\") \\n    and e.getQualifier().getType().hasName(\\"StringBuilder\\")\\n    and part = e.getArgument(0)\\n    and (part.toString().matches(\\"%SELECT Description, SupplierName, BankAccountName, ValidFrom, ValidTo%\\")\\n       or part.toString().matches(\\"%FROM Purchasing.PurchaseOrders%\\")\\n       or part.toString().matches(\\"%inner join Purchasing.PurchaseOrderLines%\\")\\n       or part.toString().matches(\\"%inner join Purchasing.Suppliers%\\")\\n    )\\nselect e.getFile()\\n```\\n\\nAnd we get one file which is correct. CodeQL is purely declarative that allows to treat AST element like a table in \\nitself.\\n\\n```\\nfrom \\n    MethodCall e,\\n    Expr part \\n```\\n\\nFind any method call and any expression in the database.\\n\\n`e.getMethod().hasName(\\"append\\")` - we are targeting appends\\n\\n`e.getQualifier().getType().hasName(\\"StringBuilder\\")` - the target type is StringBuilder \\n\\n`part = e.getArgument(0)` - limit part expression to be first argument of our append\\n\\n`part.toString().matches(\\"%FROM Purchasing.PurchaseOrders%\\")` - first argument serialized to string has this label\\n\\n`e.getFile()` - get enclosing file\\n\\n\\nNow that we found something let\'s refine the query to be more accurate.\\n\\n## Exact solution\\n\\nSo far we used pure AST based queries, but a nice feature of **CodeQL** is that allows to analyze data flow\\ninside the codebase. It\'s has it\'s limitations though - you can\'t just compute the expression for a method for\\nexample as that would mean running the program. You can however order instructions in codebase in happens-before\\nrelation which is good enough to refine our query.\\n\\nCodeQL has 2 variants: Local DataFlow and Global DataFlow. It turns out that local variant is presupposed to be \\nused on simple expressions and does not track variables across method calls. So we will use global option (I have\\nno idea how long it runs for medium sized projects)\\n\\n\\n```\\nimport java\\nimport semmle.code.java.dataflow.DataFlow\\n\\n\\n  module FindQueryConfig implements DataFlow::ConfigSig {\\n    predicate isSource(DataFlow::Node source) {\\n      exists (MethodCall jdbcQuery, MethodCall toString |\\n            jdbcQuery.getMethod().hasName(\\"query\\") and\\n            jdbcQuery.getArgument(0) = toString and \\n            toString.getMethod().hasName(\\"toString\\") and\\n            DataFlow::localFlow(DataFlow::exprNode(source.asExpr()), \\n                 DataFlow::exprNode(toString.getQualifier()))\\n        )\\n    }\\n  \\n    predicate isSink(DataFlow::Node sink) {\\n       sink.asExpr().(MethodCall).getMethod().hasName(\\"append\\")\\n       and exists (Expr argument |\\n            argument = sink.asExpr().(MethodCall).getArgument(0)\\n            and (argument.toString().matches(\\"%SELECT Description, SupplierName, BankAccountName, ValidFrom, ValidTo%\\")\\n            or argument.toString().matches(\\"%FROM Purchasing.PurchaseOrders%\\")\\n            or argument.toString().matches(\\"%inner join Purchasing.PurchaseOrderLines%\\")\\n            or argument.toString().matches(\\"%inner join Purchasing.Suppliers%\\"))\\n        )\\n    }\\n  }\\n  \\n  module FindQuery = DataFlow::Global<FindQueryConfig>;\\n  \\n  from DataFlow::Node src, DataFlow::Node sink\\n  where FindQuery::flow(src, sink)\\n  select src, \\"This environment variable constructs a URL $@.\\", sink, \\"here\\"\\n```\\n\\nIn essence dataflow is represented as ordered instructions coming from source to sink. There is only \\none sink and from it all preceding instructions are considered. Then these 2 variables can be further\\nmatched in query to limit the search.\\n\\nThis query is considering as sink any expression that ultimately lands in `jdbc.query` as an argument so \\n\\n```\\nStringBuilder aside = new StringBuilder();\\naside.append(\\"Hello \\");\\naside.append(\\"World\\");\\naside.append(\\"!\\");\\n```\\n\\nthis won\'t be matched as false positive.\\n\\nThe sink is any append expression that takes familiar labels from the previous part.\\n\\n`predicate` - this is the function that takes any arguments and returns boolean, it is \\ndesigned to offload heavy logic from your main query to selfcontained part\\n\\n`exists` - it is a subquery that matches if any of the records exists, before the bar `|`\\nyou write \\"free variables\\" and after some condition like in predicate\\n\\nThen final lines is the flow execution. Little nuisance is that instead of single file it will\\ngive you all code paths to the sinks. But clicking any of the links takes you to the file anyway.\\n\\n## Conclusion\\n\\nCodeQL is rich and mature tool. I only uncovered the tip of an iceberg for a specific use case.\\nIf you are interested you can:\\n\\n* read the documentation or github blog posts\\n* read index of all combinators\\n* read security rules written on top of it\\n\\n**Further reading**\\n\\n* [CodeQL zero to hero: part 1](https://github.blog/developer-skills/github/codeql-zero-to-hero-part-1-the-fundamentals-of-static-analysis-for-vulnerability-research/)\\n* [CodeQL zero to hero: part 2](https://github.blog/developer-skills/github/codeql-zero-to-hero-part-2-getting-started-with-codeql/)\\n* [CodeQL zero to hero: part 3](https://github.blog/security/vulnerability-research/codeql-zero-to-hero-part-3-security-research-with-codeql/)"},{"id":"/2025/03/3/","metadata":{"permalink":"/blog/2025/03/3/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-03-3.md","source":"@site/blog/2025-03-3.md","title":"Wring simple parser with Megaparsec in Haskell","description":"There goes around opinion that pure functional languages are rock solid and well suited for critical systems.","date":"2025-03-03T00:00:00.000Z","tags":[{"inline":true,"label":"haskell","permalink":"/blog/tags/haskell"},{"inline":true,"label":"parsing","permalink":"/blog/tags/parsing"},{"inline":true,"label":"programming","permalink":"/blog/tags/programming"}],"readingTime":10.715,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Wring simple parser with Megaparsec in Haskell","tags":["haskell","parsing","programming"]},"unlisted":false,"prevItem":{"title":"Semantic search for dynamically built queries in Java and CodeQL","permalink":"/blog/2025/03/9/"},"nextItem":{"title":"Introducing concurrency solver","permalink":"/blog/2025/01/18/"}},"content":"There goes around opinion that pure functional languages are rock solid and well suited for critical systems.\\nFor example Facebook uses it in anti-spam filters, serval financial companies for derivative modelling\\nand there is also some documented usage in compilers.\\n\\nI tiptoed in Haskell long time ago, but didn\'t really get it. This time, my particular usecase was that I wanted to have parser for \\ntoy language with minimal effort. Parser combinators like Parsec or Megaparsec are known for purely declarative approach \\nto modelling grammars. \\n\\nAfter 2 weeks of playing with the language I must say that there is something strangely addictive in writing pure functional \\ncode. Reading it is hard, writing it even harder, but when it starts to work there is a lot of satisfaction. I don\'t know \\nmaybe I wasn\'t feeling confident about it before, but I finally started to like it.\\n\\n\x3c!-- truncate --\x3e\\n\\nIn this short writing we are going to write simple application that reads in json, validates it and then pretty prints to \\nthe console. I used Megaparsec because its newer and has better error messages, but 99% could be written in parsec as well. \\nOn the fly I will showcase some Haskell idioms I learned on the way.\\n\\n## Normal workflow \\n\\nIn haskell every function is a pure function, which means it\'s idepotent. Side effects like IO are nicely contained in scope in \\nmonads. What you get is that because there is no mutable state all the work is really composition of functions. It\'s a little bit \\nintellectually demanding, but once you wrote it right, it\'s hard to see it failing.\\n\\nMost of the testing happens in REPL. Once you set up Stack or Cabal project you write functions in library module and the load it in REPL and \\ntest the functions.\\n\\n```\\nstack ghci\\nghci> \\n```\\n\\nFrom there you can use various tools but most helpful is probably checking types of the functions.\\n\\n```\\nghci> import Data.List\\nghci> :t concat\\nconcat :: Foldable t => t [a] -> [a]\\n```\\n\\nThen you can test the function directly in REPL \\n\\n```\\nghci> concat [\\"Hello\\", \\" \\", \\"World\\"]\\n\\"Hello World\\"\\n```\\n\\nThere are 2 things at play here in this example:\\n* haskell has parameteric polymorphism, which means you can have some abstract type a (like generics)\\n* string is actually list of char [Char]\\n\\nIf you are stubborn you can of course write tests in HUnit, but since you can test it in REPL I don\'t think \\nanyone writes them. But several libraries exist like HUnit. \\n\\n## Representing data \\n\\nHaskell has no oop concepts so everthing is represented as Algebraic Data Types (or Generalized Algebraic Data Types for more power).\\nADTs have two flavors:\\n* Sum types A or B\\n* Product types tuples (A, B)\\n\\nThis is how I represented internal JSON structure \\n\\n```\\ndata VNumber = VInt Integer |  VDouble Double\\n    deriving (Show, Eq)\\n\\ndata VJson =\\n    VNumber VNumber\\n    | VString String\\n    | VBool Bool\\n    | VNull\\n    | VArray [VJson]\\n    | VObject [(String, VJson)]\\n    deriving (Show, Eq)\\n```\\n\\nVJSON is sum type of couple of variants. It think it\'s self explanatory, except the deriving part. Since we are building on \\ntop of built in types like Integer, Double and String, typeclass Show (printing to console) and Eq can be autogenerated. Typeclasses \\nfor now are like constraints on wildcard generic types the expose some behaviour.\\n\\n## Parser \\n\\nAll parsing happens inside monad Parsec.\\n\\n```\\nParsec e s a \\n```\\nwhere \\ne - custom exception, \\n\\ns - stream type, can be String, Text, or ByteString, \\n\\na returned structure. \\n\\nso for this simple parser I created type alias \\n\\n```\\ntype Parser = Parsec Void String\\n```\\n\\nYou may notice the last type on the rhs is not defined so it \\"floats\\" to the definition on the left.\\n\\nThe flow is that you compose the smaller parsers into bigger parsers up to the top and the run topmmost one \\nwith function parse \\n\\n```\\nghci> :t parse\\nparse\\n  :: Parsec e s a -> String -> s -> Either (ParseErrorBundle s e) a\\n```\\n\\nSo it takes our parser, debugging message, text to parse and returns either Left error or Right result.\\n\\nFor now let\'s see how the main function looks like:\\n\\n```\\nrun :: IO ()\\nrun  = do \\n    args <- getArgs\\n    case args of \\n        [filePath] -> do \\n            content <- readFile filePath\\n            let parseResult = parse value \\"\\" content \\n            case parseResult of \\n                Left err -> putStr $ errorBundlePretty err \\n                Right v -> putStr $ prettyJson v 0\\n        _ -> putStr \\"Usage: program <filepath> \\\\n\\"\\n```\\n\\nPattern matching is easy to guess but for people unfamiliar with monads I will draw an analogy \\nfor do notation in Java. So you have a method to that for each element returns a stream \\n\\n```java\\nStream<Integer> foo(Integer i) {\\n    return Stream.of(i, i+1);\\n}\\n```\\n\\nAnd then you have a stream of 2 elements.\\n\\n```\\nStream nums = Stream.of(1,3)\\n```\\n\\nAnd finally you may recall that there is a method flatMap, so combining everthing together\\n\\n```\\nStream.of(1,3).flatMap(this::foo) // == Stream.of(1,2,3,4)\\n```\\n\\nSo this is much like working with monads. The foo method works on extracted value from the previous monad \\nand returns back new monad. This particular monad has intrisinc behavior so it flattens the nested streams \\nbut the behavior itself could be anyting you want it to be. You can chain multiple foos \\nand in the end you will get back monad. \\n\\nSo do notation is much alike that it allows to chain previous monadic function to the next.\\n\\n## Parser rules \\n\\nLet\'s start by parsing trivial cases of true, false and null.\\n\\n```\\nliteral :: Parser VJson\\nliteral = do\\n    lit <- choice [C.string \\"true\\", C.string \\"false\\", C.string \\"null\\"]\\n         <?> \\"only \'true\', \'false\' and \'null\' literals are valid\\"\\n    _ <- notFollowedBy C.alphaNumChar <?> \\"only \'true\', \'false\' and \'null\' literals are valid\\"\\n    if lit == \\"true\\" then\\n        return $ VBool True\\n    else if lit == \\"false\\" then\\n        return $ VBool False\\n    else\\n        return VNull\\n```\\n\\nFrom the first parts:\\n\\n```\\nC.string \\"true\\"\\n```\\nreturns a parser that takes up \\"true\\" from the input or fails if not met.\\n\\n```\\nchoice [string \\"a\\", string \\"b\\"]\\n```\\nreturns composite parser that if string \\"a\\" fails then follows to the next parser up to exhaustion.\\n\\nThen we save result in variable. \\n\\n`<?>`  allows for customizing failure message if the whole rule fails. It\'s like returning parser monad that always \\nfails with this message. \\n\\nFinally we make sure that literals have no trailers which allows us to fail exactly in this place not in some time later.\\n\\nOne of the monad methods is return which just wraps a value in the monad\\n\\n```\\nreturn $ VBool False\\n```\\n\\nThe `$` means evaluate what is on the right side and apply it to left side. It\'s needed here because evaluation happens from left to right.\\n\\nNext \\n\\n```\\nstringLiteral :: Parser VJson\\nstringLiteral = do\\n    _ <- C.char \'\\"\'\\n    content <- many (chunk \\"\\\\\\\\\\\\\\"\\" <|> (:[]) <$> anySingleBut \'\\"\')\\n    _ <- C.char \'\\"\'\\n    return $ VString (concat content)\\n```\\n\\nFirst match \'\\"\' single character then take zero or more parts that\\n\\n```\\nchunk \\"\\\\\\\\\\\\\\"\\" <|> (:[]) <$> anySingleBut \'\\"\'\\n```\\nMatches 2 chars \\\\ and \\" if that fails try parsing any char that is not \\". \\n\\nOne word of explanation for `<$>`. This operator takes a function (conversion to list or appending empty list afterwards)\\nand applies it inside the monad resulting in new monad with the result from function.\\n\\nThe most hard is parsing number \\n\\n```\\nzero :: Parser String\\nzero = C.string \\"0\\"\\n\\ndigitFromOne :: Parser Char\\ndigitFromOne = oneOf [\'1\'..\'9\']\\n\\nwholePart :: Parser String\\nwholePart = choice [\\n    zero,\\n     ((++) . (: []) <$> digitFromOne) <*> many C.digitChar]\\n\\nfractPart :: Parser String\\nfractPart  = do\\n    dot <- C.char \'.\'\\n    fract <- some C.digitChar\\n    return $ dot : fract\\n\\nexpPart  :: Parser String\\nexpPart = do\\n    e <- oneOf [\'E\', \'e\']\\n    sign <- option \\"\\" ((:[]) <$> oneOf [\'+\', \'-\'])\\n    num <- some C.digitChar\\n    return $ e : (sign ++ num)\\n\\n\\n\\nnumber :: Parser VJson\\nnumber = do\\n    sign <- option \\"\\" ((:[]) <$> oneOf [\'+\', \'-\'])\\n    wh <- wholePart <?> \\"whole part of number\\"\\n    fract <- option \\"\\" fractPart\\n    e <- option \\"\\" expPart\\n    if null fract  && null e then\\n        return $ VNumber (VInt $ read (sign ++ wh))\\n    else\\n        return $ VNumber (VDouble $ read (sign ++ wh ++ fract ++ e))\\n```\\n\\nExplanations:\\n\\n```\\n((++) . (: []) <$> digitFromOne) <*> many C.digitChar]\\n```\\nFirst convert char to String (with single char) inside monad. Then take this result and concat it with string \\nparsed by `many`.\\n\\n\\n```\\nghci> :t option\\noption :: GHC.Base.Alternative m => a -> m a -> m a\\n```\\n\\nSo option takes a default value, and a monad return new monad with possibly default applied. \\n\\nSo for example what this does \\n\\n```\\ne <- option \\"\\" expPart\\n```\\n\\nis that if `expPart` matches it returns string otherwise empty string \\"\\" is assigned. \\n\\nThen array value \\n\\n```\\narray :: Parser VJson\\narray = do\\n    _ <- C.char \'[\'\\n    arr <- value `sepBy` C.char \',\'\\n    _ <- C.char \']\'\\n    return $ VArray arr\\n```\\n\\nAnd object \\n\\n```\\nkeyValuePair :: Parser (String, VJson)\\nkeyValuePair = do\\n    C.space\\n    VString key <- stringLiteral\\n    C.space\\n    _ <- C.char \':\'\\n    v <- value\\n    return (key, v)\\n\\n\\nobject :: Parser VJson\\nobject = do\\n    _ <- C.char \'{\'\\n    kvs <- keyValuePair `sepBy` C.char \',\'\\n    _ <- C.char \'}\'\\n    return $ VObject kvs\\n```\\n\\nNow that we have all the rules it is time to combine them \\n\\n```\\nvalue :: Parser VJson\\nvalue = do\\n    C.space\\n    v <- try literal <|> try stringLiteral <|> try number <|> try array <|> object\\n    C.space\\n    return v\\n```\\n\\n`try` tries parsing rule without consuming the input, note the last alternative is without try \\nto guarantee progress. It is a nice trick to consume spaces before and after proper value.\\n\\n\\nMegaparsec include test method parseTest. Let\'s try it:\\n\\n```\\nghci> parseTest value \\"{\\\\\\"a\\\\\\" : 1, \\\\\\"b\\\\\\" : [1.0, 2.0]}\\"\\nVObject [(\\"a\\",VNumber (VInt 1)),(\\"b\\",VArray [VNumber (VDouble 1.0),VNumber (VDouble 2.0)])]\\n```\\n\\n## Printing \\n\\nSome branches are easy but some are complicated:\\n\\n```\\nprettyJson :: VJson -> Int -> String\\nprettyJson VNull _ = \\"null\\"\\nprettyJson (VBool True) _ = \\"true\\"\\nprettyJson (VBool False) _ = \\"false\\"\\nprettyJson (VNumber (VInt i)) _ = show i\\nprettyJson (VString s) _ = \\"\\\\\\"\\" ++ s ++ \\"\\\\\\"\\"\\nprettyJson (VNumber (VDouble d)) _ = show d\\nprettyJson (VArray arr) ind = \\"[\\" ++ printElems arr (ind+2) ++ \\"\\\\n\\" ++ indent ind \\"]\\\\n\\"\\n    where \\n        printElems (a:as) i = foldl \\n            (\\\\acc x -> acc ++ \\",\\\\n\\" ++ indent i \\"\\" ++ prettyJson x i) (\\"\\\\n\\" ++ indent i \\"\\" ++ prettyJson a i) as\\n        printElems [] _ = \\"\\"\\n        indent i s = replicate i \' \' ++ s\\nprettyJson (VObject arr) ind = \\"{\\" ++ printElems arr (ind+2) ++ \\"\\\\n\\" ++ indent ind \\"}\\\\n\\"\\n    where \\n        printElems ((k, v):as) i = foldl \\n            (\\\\acc (k2, v2) -> acc ++ \\",\\\\n\\" ++ indent i \\"\\" ++ \\"\\\\\\"\\" ++ k2 ++ \\"\\\\\\" : \\" ++ prettyJson v2 i) \\n            (\\"\\\\n\\" ++ indent i \\"\\" ++ \\"\\\\\\"\\" ++ k ++ \\"\\\\\\" : \\" ++ prettyJson v i) \\n            as\\n        printElems [] _ = \\"\\"\\n```\\n\\nLet\'s only concentrate on VArray branch \\n\\n```\\nprettyJson (VArray arr) ind = \\"[\\" ++ printElems arr (ind+2) ++ \\"\\\\n\\" ++ indent ind \\"]\\\\n\\"\\n    where \\n        printElems (a:as) i = foldl \\n            (\\\\acc x -> acc ++ \\",\\\\n\\" ++ indent i \\"\\" ++ prettyJson x i) (\\"\\\\n\\" ++ indent i \\"\\" ++ prettyJson a i) as\\n        printElems [] _ = \\"\\"\\n        indent i s = replicate i \' \' ++ s\\n```\\n\\n`printElems` is a helper method that prints elements each in new and indented line. The whole trick was that to not include \\n\\",\\" after last expression. So this is how i did it. First match a list with first element and the rest. Then use `foldl` to reduce \\narray of json values to one string. The starting value is the matched first element `a` and it\'s different. Then reduction function takes \\naccumulator and appends next element with \\",\\" prepended. And for empty array just return empty string. `indent` is a helper method that prefixes \\nthe give string with number of spaces. \\n\\nLet\'s test:\\n\\n```\\nghci> putStr $ prettyJson ( VArray [(VNumber (VInt 1)), (VNumber (VInt 2))]) 0\\n[\\n  1,\\n  2\\n]\\nghci> \\n```\\n\\n## Conclusion \\n\\nThat\'s it  for this simple tutorial. Here is a list of nice readings \\n\\n* [Megaparsec intermediate tutorial](https://markkarpov.com/tutorial/megaparsec.html#notfollowedby-and-lookahead)\\n* [What I whish I knew learning Haskell](https://smunix.github.io/dev.stephendiehl.com/hask/index.html)\\n* [GHC reading list | advanced ](https://gitlab.haskell.org/ghc/ghc/-/wikis/reading-list)\\n* [Write you a haskell](https://smunix.github.io/dev.stephendiehl.com/fun/index.html)\\n\\nI had a lot of fun and eager for new challenges. If I have some spare time maybe I will write how to write System F typechecker \\nin Haskell.\\n\\nThanks for now.\\n\\nPS Here is the [code](https://github.com/vulture-dominiczek/haskell-json-parser)."},{"id":"/2025/01/18/","metadata":{"permalink":"/blog/2025/01/18/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-01-18.md","source":"@site/blog/2025-01-18.md","title":"Introducing concurrency solver","description":"Lately at work most of the staff is puzzled with mysterious bug. In short there is a statemachine that","date":"2025-01-18T00:00:00.000Z","tags":[{"inline":true,"label":"microservices","permalink":"/blog/tags/microservices"},{"inline":true,"label":"distributed","permalink":"/blog/tags/distributed"}],"readingTime":3.14,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Introducing concurrency solver","tags":["microservices","distributed"]},"unlisted":false,"prevItem":{"title":"Wring simple parser with Megaparsec in Haskell","permalink":"/blog/2025/03/3/"},"nextItem":{"title":"My exploration of WASM/WASI","permalink":"/blog/2025/01/10/"}},"content":"Lately at work most of the staff is puzzled with mysterious bug. In short there is a statemachine that\\nprocesses movements in batches. But sometimes one particular movement is duplicated and nobody knows why...\\n\\nI wish I could brag I solved it myself, but that is not the case. But it inspired me to dig a little bit in \\ntheory how distributed systems/concurrency is reasoned about and visualized.\\n\\n\\n## Time space diagrams\\n\\nI have read about them in some book long ago and was looking for some time find the correct name. It\'s pretty niche\\nconcept, but in my opinon unjustly. They are so good to visualize not only distributed systems but also concurrency.\\n\\nLet me show you.\\n\\n![](/img/synchronization.png)\\n\\n\x3c!-- truncate --\x3e\\n\\nThis is the basic case of dirty read, var a is not lock and therefore instead of being increamented it is wrote twice\\nwith same value 2. The example is pretty silly but you can imagine more complex scenarios where the drawing would come in handy.\\n\\n### Terminology\\n\\nSo you can imagine that the horizontal lines and crossing arrows enforce partial ordering of the events. In cases of multithreading there is no\\nmore to add but in case of distributed systems time flies different on each of the actors line.\\n\\n**The cut** is any line cutting the diagram in half so that the partial ordering of the events is preseved. You can think of it as a snapshot or\\nan instant in systems run. And it has associated with it a **global state** that corresponds to any variables on any actor alltogether.\\n\\n**The run** is a particular combination of actions runs **while preserving the partial order**. So you remember that one thread can be faster in one \\ngo and in other go it can be faster than the others for instance.\\n\\nIf we had two threads and their actions lined up next to each other, then at each increment we would have 3 options: \\n\\n* thread 1 executes, thread 2 waits\\n* thread 1 waits, thread 2 executes\\n* both execute\\n\\nSo all possible runs would be 3^n with n pairs of actions. This is **the lattice** and from it we can generate all valid runs.\\n\\n### Introducing Octopus\\n\\nSo I thought - instead of instrumenting java code and what not, why not write little modelling tool in python. And reason about the correct concurrency\\npatters by enummerating all of the runs.\\n\\nThe tool that I wrote has one shared dictionary and multiple threads \\"processes\\". Processes interact with the shared state by several primitives:\\n\\n* read at path\\n* write at path\\n* compute next local state on old local state\\n* lock at path\\n* unlock at path\\n* noop (for testing purposes)\\n\\nIt turns out that locking,writing and reading on XPath is really powerful and I think you can model with it any concurrency problem I know - for example\\nDining Philosophers.\\n\\nSo [here](https://github.com/vulture-dominiczek/octopus) it is.\\n\\nTo tease it up a little bit modelling looks like this:\\n\\n```python\\n\\nimport os, sys\\n\\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\\n\\n\\nfrom core import DataTransfer\\nfrom api import Octo\\nfrom config import PrintOpts\\n\\n# PrintOpts.PRINT_LOCALS = False\\n\\nOcto.init_shared_state({ \'a\': 1})\\n\\np1 = Octo.process(name = \'Alice\')\\np2  = Octo.process(name = \'Bob\')\\n\\n\\n\\ndef incA(locals):\\n    locals[\'a\'] += 1\\n\\np1.read([DataTransfer(from_path=\'a\', to_path=\'a\')])\\np1.compute(incA, description=\'Increment a\')\\np1.write([DataTransfer(from_path=\'a\', to_path=\'a\')])\\n\\n\\np2.read([DataTransfer(from_path=\'a\', to_path=\'a\')])\\np2.compute(incA, description=\'Increment a\')\\np2.write([DataTransfer(from_path=\'a\', to_path=\'a\')])\\n\\nOcto.solve_lattice(output=\'output.txt\', validity_check={\'a\': 3})\\n```\\n\\n\\nAbout aforementioned problem here is solver output [without locks](/out1.txt).\\n\\nAnd here is with [locks](/out2.txt).\\n\\nI wish I also add messaging to the tool if I have more time in the future.\\n\\n### Conclusion\\n\\nTime flies when you are having fun. If I had to reason about concurrency issues I would use such tool I just created to ennumerate all the runs\\nand print out where the issue is.\\n\\nThanks"},{"id":"/2025/01/10/","metadata":{"permalink":"/blog/2025/01/10/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2025-01-10.md","source":"@site/blog/2025-01-10.md","title":"My exploration of WASM/WASI","description":"Assembler was developed in 1947, wow! It makes 78 years of computing development in which we saw higher level programming languages, virtual machine programming languages (write once run everywhere), virtual machines, cloud and so on. A lot of knowledge got accumulated over the time which you can see in size of artifacts deployed to cloud.","date":"2025-01-10T00:00:00.000Z","tags":[{"inline":true,"label":"wasm","permalink":"/blog/tags/wasm"},{"inline":true,"label":"microservices","permalink":"/blog/tags/microservices"},{"inline":true,"label":"fermyonspin","permalink":"/blog/tags/fermyonspin"}],"readingTime":3.97,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"My exploration of WASM/WASI","tags":["wasm","microservices","fermyonspin"]},"unlisted":false,"prevItem":{"title":"Introducing concurrency solver","permalink":"/blog/2025/01/18/"},"nextItem":{"title":"Christmas with Quantum Mechanics","permalink":"/blog/2024/12/29/"}},"content":"Assembler was developed in 1947, wow! It makes 78 years of computing development in which we saw higher level programming languages, virtual machine programming languages (write once run everywhere), virtual machines, cloud and so on. A lot of knowledge got accumulated over the time which you can see in size of artifacts deployed to cloud.\\n\\nOver the years we have seen several attempts reach for the roots. Similarly **WebAssembly** is instruction format for virtual machine designed to be portable compilation target for any language willing. It is fast as nearly native speed, secure and sandboxed and language agnostic. Originally wrote for the browsers it is beginning to get traction as microservice runtime, which btw is topic of this post.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Comparison to JVM\\n\\nIt is obvious that WASM and JVM share similarities as intermediate-represented runtimes. If you have interpreted language then out of necessity you have to have some mechanism to speed things up. For Java\\nthere is tiered JIT compiler and Wasm also has JIT however there is also possiblity of Ahead of Time Compilation. \\n\\nWasm was designed for *security, portability, and fast startup* and differs in several things from Java. \\nFirst of all WASM modules are compact binaries and smaller size means faster loading. Second all the dependencies are pre-bundled and not needed to be loaded dynamically which takes time. And the binary code is already sandboxed which means it does not need to be validated.\\n\\nBut what is most interesting is that WASM bytecode is closer to native binaries than JVM bytecode which means it can be JITed with less effort. Here is why:\\n\\n* linear memory model similar to C-style pointers\\n* no GC\\n* basic numeric types\\n* assembly like control flow (loops, branches, direct-calls) versus method invocations\\n\\nIt looks like history took whole circle.\\n\\n## Why bother\\n\\nWhere Wasm will shine in near future is serverless computing, which is now more or less poinless. Imagine deploying 100MB artifact which will spin up in 5 seconds for only ONE function. But with Wasm and it\'s promises to start withing 20ms the task takes whole new dimension.\\n\\nAnd of course cloud is money. And where is money (cost savings) there imo will always be political will to move things forward. I\'m pretty sure WebAssembly has bright future, at least for the cloud and edge.\\n\\nA month ago I watched [this wasmCloud presentation](https://www.youtube.com/watch?v=fQdkNGZqYZA) which was an interesting case for the Edge. Imagine that you run a factory with devices with computing constraints. If the gear is spinning 5000 RPMs you have to take corrective action right now, instead of waiting for server to respond. Wasm can be deployed on such machines (Edge) and take necessary actions of control loop while delegating higher level business functions to cloud.\\n\\n### Sandboxing and Formal Verification\\n\\nIf computer programs were computationally proven to be correct (formal verification) then there would be no bugs at all. Sandboxing is one step close towards this goal. If you enforce clear boundaries of the system, \\nwhich files are used, what is sent on network and so on the security is much easier to reason about without \\"gaps\\". And once you have perfectly isolated component without gaps you can use formal tools to actually prove the logic is correct. I found [something here](https://oa.upm.es/75802/1/TFM_DAVID_MUNUERA_MAZARRO.pdf) will take a look in near future.\\n\\n### WebAssembly Component Model\\n\\nSo far the application have been run in one-off basis, but standards are emerging which will handle component interactions (microservices). Here is a [summary](https://component-model.bytecodealliance.org/).\\n\\n\\n|Word|Definition|\\n|----|----------|\\n|component| a specially wrapped wasm binary that can interact with other components on clear interfaces|\\n|interface| a contract between components that is language agnostic|\\n|WIT| interface defnition langauge that you port with componentized applications|\\n|world| contract between component and runtime|\\n\\nSo you basically write the functionality in Rust and **export** functions in Wit. Other component **imports** (also with WIT) these functions and the serialization/deserialization is handled by runtime. wasmCloud claims to also remotely call the imports which is handled with NATS. Why WIT you may ask? Because wasm primtives are just numbers and strings. For high level functionality you need *records*, *tuples*, *variants*, *lists* and so on.\\n\\n### For now disapointing\\n\\nIt looks all good but is still in it\'s nascency. This means you probably won\'t see library for your use (if you don\'t write yourself). However there are several runtimes that are under development:\\n\\n* wasmCloud\\n* fermyon/spin\\n* wasmEdge\\n* WAGI\\n* and more\\n\\nRight now it is hard to host microservices application with WASI other than communicating via http.\\n\\nI got carried away and wrote some [number factorization microservice app](https://github.com/vulture-dominiczek/fermyon-spin-eval). Check it out!\\n\\n\\n### Conclusion\\n\\nIt\'s worth following the developments, that\'s for sure. I\'m pretty certain that the bright future is ahead for WASM."},{"id":"/2024/12/29/","metadata":{"permalink":"/blog/2024/12/29/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-12-29.md","source":"@site/blog/2024-12-29.md","title":"Christmas with Quantum Mechanics","description":"Christmas is wonderful time, although not without challenges. Particular challenge for me is to keep mind fresh and not sleep all the time. I like the atmospehere of going to saint masses and eating good food (with couple of drinks) and stuff but as always I felt a little bit lazy.","date":"2024-12-29T00:00:00.000Z","tags":[{"inline":true,"label":"systems","permalink":"/blog/tags/systems"},{"inline":true,"label":"math","permalink":"/blog/tags/math"},{"inline":true,"label":"quantum","permalink":"/blog/tags/quantum"}],"readingTime":3.56,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Christmas with Quantum Mechanics","tags":["systems","math","quantum"]},"unlisted":false,"prevItem":{"title":"My exploration of WASM/WASI","permalink":"/blog/2025/01/10/"},"nextItem":{"title":"Thoughts on observability","permalink":"/blog/2024/12/15/"}},"content":"Christmas is wonderful time, although not without challenges. Particular challenge for me is to keep mind fresh and not sleep all the time. I like the atmospehere of going to saint masses and eating good food (with couple of drinks) and stuff but as always I felt a little bit lazy.\\n\\nSo to exercise my brain a little I found out [this course in quantum computation](https://learning.quantum.ibm.com/). It was great fun in short. I was constantly on the edge of my cognitive possiblities, but the material was made crystal clear. I developed a particular method to finish one module at the time in the afternoon and rehearse reading part during mornings. I had a little bit of crysis on first day of Christmas but managed to carry on half consciously.\\n\\nAnd finally on the return home I passed the test with 75% score.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Foundations are key\\n\\nI was surprised that the boring linear algebra stuff I studied on freshman year on univesity actually makes a big difference. In fact 3/5 of the material is just plain algebra. I was also shocked how much I remebered. I think that because quantum setting is relatively new people try to comprehend it by analogy to classical settings. There is a huge overlap between mathematical model of classical and quantum mechanics.\\n\\nThe quantum operations are basically multiplication of state vectors by operation matrices (with particular properties). There is also place for for Bayes Theorem when measuring the quantum state.\\n\\nI liked the matrial for one particular reason: the intuition goes first. The math apparatus are just a means to succintly and without confusion describe simple concepts. The course showed a huge effort to develop mathematical feeling when describing classical and quantum state.\\n\\nEach module builds on the previous so if you suck at one particular part, you won\'t be able to fully understand the next parts. The most difficult part for me was to imagine operations on matrices and state vectors. Also around the module 2-3 the atmosphere gets dense and you have to be in shape to ingest the material.\\n\\n## Humble beginnings\\n\\n\\n\\nRecently Google unveiled their quantum chip Willow with 105 qbits. Well I don\'t even imagine somebody crossing the barrier of 1000 qubits. I mean the math is so advanced that for circuits with 2 qbits is hard to understand enough.\\n\\nBut I guess with time the same will happen as with classical chips. Somebody will develop apt abstractions on top of this advanced math so that programming will be possible on higher level. I wonder what that language will look like. Keyword entangle(a,b)? Possibly. Or maybe p_measure(pi, X, Y)? Anyways today state of art is just like classical chips 60 years ago.\\n\\n## But still improvement\\n\\nOver the time of course I had a possiblity to learn what makes quantum computation special. It is this entanglement property that has several astonishing uses.\\n\\nFirst the quantum teleportation. It\'s not like teleporting the matter as in television, but rather it is a protocol. So if you have 2 qbits in entangled state you can instantly (faster than light) switch on qbit and the second one sets different. But after that - poof the first qbit is gone as well as share e-bit. And here is the disappointing part - even if the information does really teleport, then you still have to send the classical bit by classical means (slower than speed of light) or you won\'t be able to read it.\\n\\nSecond there is superdense coding that allows to exchange one qbit with 2 classical bits of information, but on the condidtion that you have e-bit (entangled two qbits). And this operation uses e-bit without coming back.\\n\\nAnd finally when you have entangled qbits you can beat upper limits for classical strategies for cooperative games like CHSH game. To have an entangled qbit is like having agreed on particular strategy between players without communication. Interesting...\\n\\n## So much to discover...\\n\\nThere are more courses from IBM on quantum so maybe when I have time I will proceed with next modules. But for now a brief pause.\\n\\nThis is fascinating branch of science and who knows what will come next. From this time on I will follow the news more closely.\\n\\nThanks."},{"id":"/2024/12/15/","metadata":{"permalink":"/blog/2024/12/15/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-12-15.md","source":"@site/blog/2024-12-15.md","title":"Thoughts on observability","description":"Everything is complicated, even those things that seem flat in their bleakness.","date":"2024-12-15T00:00:00.000Z","tags":[{"inline":true,"label":"systemdesign","permalink":"/blog/tags/systemdesign"},{"inline":true,"label":"cloud","permalink":"/blog/tags/cloud"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":3.635,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Thoughts on observability","tags":["systemdesign","cloud","kubernetes"]},"unlisted":false,"prevItem":{"title":"Christmas with Quantum Mechanics","permalink":"/blog/2024/12/29/"},"nextItem":{"title":"Reflections after writing simple Spring Boot library","permalink":"/blog/2024/11/5/"}},"content":"*Everything is complicated, even those things that seem flat in their bleakness.*\\n\\nDebugging microservices application based on scarce information is one of those cases that I don\'t wish\\nanyone. But it is how it is at my current project, so management started to put some measures in motion. \\n\\nI reaserched topic a bit at work and a bit on my own and I have something to share - [OpenTelemetry is the future](https://opentelemetry.io/). Bu it is still work in progress.\\n\\nIn this post I will tell you everything I learned.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Observability intro\\n\\nWhen it comes to first (and most important) pillar of observability - logs, the first little revolution came with the invention of **Mapped Diagnostic Context**.  Neil Harrison described this method in the book *Patterns for Logging Diagnostic Messages in Pattern Languages of Program Design 3, edited by R. Martin, D. Riehle, and F. Buschmann (Addison-Wesley, 1997)*. \\n\\nThe beauty of the idea is it\'s simplicity. You put collection of key value pairs in thread local storage and \\nimplicitly append them to logs each time the log message is created. As you know ids, names and other stuff can \\"span\\" multiple nested method calls and this really makes life a little bit easier. Beacause it 2025 all logging framework support it, but that\'s not all.\\n\\nTaking it one step further you could propagate this key values across threads and (eventually) service boundaries. Welcome to distributed tracing. Spring Boot covers all this stuff semi-automatically all you have to do is to configure proper tools (Zipkin-Brave or OTEL). \\n\\nAnd finally golden standard Prometheus scrapes at given metrics exported from your Spring app on given endpoint and acts as a store for Grafana to display. \\n\\nThese were the 3 components of Observability: Metrics, Logs and Traces.\\n\\n## What the future will bring\\n\\nNot so long ago **Open Telemetry** emerged as a standard for application instrumentation with all 3 observability components. State of providers is quite good - 40+ languages covered more or less exporting to \\nvarious sinks like ELK stack, Grafana and Zipkin/Jaeger. \\n\\nBut to be honest I didn\'t find any easy to use out of the box tool to visualize these goods. Yes there is Grafana with extensions for Loki (logs) and tempo db (traces) but the documentation has holes and you can\'t just set everything in a day. Some Open Source dashboards exist to my knowledge but they are a little bit clunky to use and limited.\\n\\nAs usual complex stuff requires time and time is money. So I would look for the innovations in the commercial side of the topic. I even have my favourite **Lighstep** - now ServiceNow Cloud Observability is one such solution that can do it well. They market it as\\n\\n* possible to go all between logs traces and metrics\\n* use AI to diagnose problems (interesting)\\n\\nBut I\'m not an architect and not planning to put my own money on the table so this has stay in the land of dreams for now.\\n\\n## How I done POC\\n\\nAt work I tried to setup Spring to send traces to Zipkin as Proof of Concept and it costed me a little bit of effort. Because there are 2 bridges (adapters) and 2 exporters there is a little bit of problem - mix any of the 2 up and it will break. Oh how I wish it would just break with explicit error! Instead it just does not send logs and go figure why. \\n\\nLuckily I started with OTEL first and when I switched I saw mixed jars so after exluding otel from parent pom i got finally traces to zipkin. The next step was to actually propagate the trace id across services boundaries. I discovered a cool feature - a **baggage**. It\'s a key-value pairs collection that propagetes with traceId/spanId. If you set some config property (like remote-fields or something) you get it sent to other service for free.\\n\\nAs long as you use Spring stack to send requests the MDC gets exported as SimpleTextMap sent in a header.\\n\\n\\n\\n## Conclusion\\n\\nFuture is integration of all three pillars of observability. \\n\\nIn time there eventually be a Open Source solution that enables you to switch between logs traces and metrics from single place and it will be to much of everyones joy.\\n\\nBut for now waiting is all we can do."},{"id":"/2024/11/5/","metadata":{"permalink":"/blog/2024/11/5/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-11-5.md","source":"@site/blog/2024-11-5.md","title":"Reflections after writing simple Spring Boot library","description":"*Sometimes learning from adversity is better than trying to avoid it.","date":"2024-11-05T00:00:00.000Z","tags":[{"inline":true,"label":"systemdesign","permalink":"/blog/tags/systemdesign"},{"inline":true,"label":"spring","permalink":"/blog/tags/spring"},{"inline":true,"label":"java","permalink":"/blog/tags/java"}],"readingTime":3.83,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Reflections after writing simple Spring Boot library","tags":["systemdesign","spring","java"]},"unlisted":false,"prevItem":{"title":"Thoughts on observability","permalink":"/blog/2024/12/15/"},"nextItem":{"title":"1BRC Challenge","permalink":"/blog/2024/10/29/"}},"content":"*Sometimes learning from adversity is better than trying to avoid it.\\nTaking it into careful consideration provides valuable lessons that will support you in the future.*\\n\\n     \\n\\nI appreciate my job for one particular thing. That is, it provides steady steam of difficult problems that challenge my intellect. Recently I tried to wrap my head around problem how to write tests for semi-large Spring Boot codebase and refactor it (with no tests whatsoever).\\n\\nI started from the assumption that when you don\'t have any legacy tests at hand first you write them. How can you know you don\'t break functionality without running the tests? But the code was very unfriendly and writing them would require writing mocks.\\n\\nSo I thought - why not automate stuff a little bit:\\n\\n* instrument given beans with reflection\\n* dump args and results to json\\n* load json directly in tests instead of writing mocks in plain Java\\n\\n\x3c!-- truncate --\x3e\\n\\n## The problem\\n\\nMain problem is that there are absolutely no tests. All testing is done by business. What a terrible waste of time... Each time something changes they click their way through all over again.\\n\\nSo I thought I will take snapshot of current state of things and secure some tests in the backend. But there is some many API methods! And each has nested call to service and service calls repository.\\n\\nI would write aspects no problem, but repositories are as you recall interfaces for which the classes are generated by Spring. And to add to that I want to move fast and writing the Advices takes time.\\n\\nI have come up with instrumenting the selected beans, but those beans can autowire another instrumented beans. So some mechanism has to be devised to order the instrumentation to do it from the bottom-most to the top.\\n\\n## Approach\\n\\n\\n\\nSo I have both \\"special\\" interfaces of repositories and plain beans annotated as components. Then you have to cater for both of the cases. For the interfaces I generated JDK dynamic proxies. For classes (as it happens they don\'t implement any interface) I used ByteBuddy instrumentation library to subclass them and load them with default classloader.\\n\\nAnd I have instrumented beans. Now I unload the old beans and switch them to proxes. For that I used BeanFactory which I took from the ApplicationContext with some class casting. But that is not all beacuse you still have to autowire all affected beans.\\n\\nFirst iteration of autowiring was very wasteful and boiled down to realoading everything. Then I came to the conclusion to build the dependency graph and updated only the nodes of the graph in reverse order. Spring BeanFactory has both methods for finding bean dependencies and dependants so it was pretty easy. One catch is that you have to actually autowire not the bean itself but the subclassed proxy. After that everything worked as charm.\\n\\nIf you are interested you can download the lib here.\\n\\n## Reflection\\n\\nAfter writing the lib and being exposed to the project at work I have some thoughts.\\n\\nFirst if you write Autoconfig lib to be used by other Spring components you absolutely can\'t poison the application context with unncessary stuff. If you you put bootstrap.yaml in your library it will float on forever and take predecence and collide with stuff already defined. Spring treats the same multiple bootstraps and in case of name collision the order is random. And if you specify it in config/- directory it will actually take predecence over anything else.\\n\\nMoreover you should not define any components picked up by component scanning as these can clash with other things defined. Just confine everything inside the @AutoConfiguration annotated class with the exception of loading necessary stuff with @Import.\\n\\nLikewise you should not include starters in your library because they will for sure accumulate for several libs. Just use specific non-umbrella deps even better include your own jars.\\n\\nIF you have crosscutting concerns like authentication, you should not use internals outside the pointcuts. So for example you have authentication fetching the token before the feign call. Then using static class for caching the token and pulling it from the business logic is a big mistake.\\n\\nFinally you should have mock database and the whole thing would not be needed. I would just happily write e2e tests then.\\n\\n## Conclusion\\n\\nWhat will be of my little library is yet to be defined. However I learned about some best practices for writing spring boot libs and learned some lessons not to repeat over again. You I am happily sharing it to you by this article."},{"id":"/2024/10/29/","metadata":{"permalink":"/blog/2024/10/29/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-10-29.md","source":"@site/blog/2024-10-29.md","title":"1BRC Challenge","description":"One thing that recently got nerd the hell out of me was 1 billion row challenge. Citing the original site:","date":"2024-10-29T00:00:00.000Z","tags":[{"inline":true,"label":"java","permalink":"/blog/tags/java"},{"inline":true,"label":"optimization","permalink":"/blog/tags/optimization"}],"readingTime":11.025,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"1BRC Challenge","tags":["java","optimization"]},"unlisted":false,"prevItem":{"title":"Reflections after writing simple Spring Boot library","permalink":"/blog/2024/11/5/"},"nextItem":{"title":"Topology graphs are important (and fun)","permalink":"/blog/2024/10/26/"}},"content":"One thing that recently got nerd the hell out of me was 1 billion row challenge. Citing the original site:\\n\\n*Your mission, should you decide to accept it, is deceptively simple: write a Java program for retrieving temperature measurement values from a text file and calculating the min, mean, and max temperature per weather station. There\u2019s just one caveat: the file has 1,000,000,000 rows!*\\n\\nI was working on it after hourse and 1 week after taking on the challenge there are several conclusions worth writing about.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Baseline\\n\\nTo make dev loop faster I decided limit the task to 100 million rows.\\n\\nThe baseline from the autor was an idiomatic streams, but single threaded.\\n\\n\\n```\\npublic class CalculateAverage_baseline {\\n\\n    private static final String FILE = \\"./measurements.txt\\";\\n\\n    private static record Measurement(String station, double value) {\\n        private Measurement(String[] parts) {\\n            this(parts[0], Double.parseDouble(parts[1]));\\n        }\\n    }\\n\\n    private static record ResultRow(double min, double mean, double max) {\\n\\n        public String toString() {\\n            return round(min) + \\"/\\" + round(mean) + \\"/\\" + round(max);\\n        }\\n\\n        private double round(double value) {\\n            return Math.round(value * 10.0) / 10.0;\\n        }\\n    };\\n\\n    private static class MeasurementAggregator {\\n        private double min = Double.POSITIVE_INFINITY;\\n        private double max = Double.NEGATIVE_INFINITY;\\n        private double sum;\\n        private long count;\\n    }\\n\\n    public static void main(String[] args) throws IOException {\\n\\n        Collector<Measurement, MeasurementAggregator, ResultRow> collector = Collector.of(\\n                MeasurementAggregator::new,\\n                (a, m) -> {\\n                    a.min = Math.min(a.min, m.value);\\n                    a.max = Math.max(a.max, m.value);\\n                    a.sum += m.value;\\n                    a.count++;\\n                },\\n                (agg1, agg2) -> {\\n                    var res = new MeasurementAggregator();\\n                    res.min = Math.min(agg1.min, agg2.min);\\n                    res.max = Math.max(agg1.max, agg2.max);\\n                    res.sum = agg1.sum + agg2.sum;\\n                    res.count = agg1.count + agg2.count;\\n\\n                    return res;\\n                },\\n                agg -> {\\n                    return new ResultRow(agg.min, (Math.round(agg.sum * 10.0) / 10.0) / agg.count, agg.max);\\n                });\\n\\n        Map<String, ResultRow> measurements = new TreeMap<>(Files.lines(Paths.get(FILE))\\n                .map(l -> new Measurement(l.split(\\";\\")))\\n                .collect(groupingBy(m -> m.station(), collector)));\\n\\n        System.out.println(measurements);\\n    }\\n}\\n```\\n\\nThe performance is not good as expected:\\n\\n```\\ntime ./calculate_average_baseline.sh \\n\\nreal    0m15,846s\\nuser    0m16,002s\\nsys     0m0,769s\\n```\\n\\n## Parallelizing the code\\n\\n\\n\\nMy first thought was to parallelize the code and see how fast it will run. Obvious choice is to use streams the same way but add .parallel() call. So updates go to central HashMap and access has to be synchronized.\\n\\nOne way is to use locks, tried it and it was slow. Remembering we have 100 million rows locking on whole object is very wasteful. Much better would be to lock only on specific hashmap entry with given key.\\n\\nWhat surprised me was that Atomic* primitives and spinlocks were actually slower than wait/notify. Luckly there is another way you can do same thing with ConcurrentHashMap. When calling compute method the run lambda will be synchronized.\\n\\nSo the code reads file on the fly and uses Spliterator to carve out overflow of work to another thread run on ForkJoinPool. Then each iteration of foreach does the update on ConcurrentHashMap.\\n\\n```\\npublic class CalculateAverage_dg2 {\\n\\n    private static final String FILE = \\"./measurements.txt\\";\\n\\n    private static class Measurement {\\n        public double min = Double.MAX_VALUE;\\n        public double max = Double.MIN_VALUE;\\n        public double sum = 0;\\n        public double count = 0;\\n\\n        public Measurement(double value) {\\n            this.min = value;\\n            this.max = value;\\n            this.sum = value;\\n            this.count = 1;\\n        }\\n\\n        @Override\\n        public String toString() {\\n            return round(min) + \\"/\\" + round(max) + \\"/\\" + round(sum / count);\\n        }\\n    }\\n\\n    public static void main(String[] args) throws Exception {\\n\\n        ConcurrentHashMap<String, Measurement> measurements = new ConcurrentHashMap<>();\\n\\n        Files.lines(Path.of(FILE)).parallel().forEach(line -> {\\n\\n            int split = line.indexOf(\\";\\");\\n            String key = line.substring(0, split);\\n            double val = Double.parseDouble(line.substring(split + 1));\\n\\n            measurements.compute(key, (k, _v) -> {\\n                if (_v == null) {\\n                    return new Measurement(val);\\n                }\\n\\n                _v.min = Double.min(_v.min, val);\\n                _v.max = Double.max(_v.max, val);\\n\\n                _v.sum += val;\\n                _v.count++;\\n\\n                return _v;\\n            });\\n        });\\n\\n        var sorted = new TreeMap<>(measurements);\\n\\n        System.out.println(sorted);\\n\\n    }\\n\\n    private static double round(double value) {\\n        return Math.round(value * 10.0) / 10.0;\\n    }\\n\\n}\\n```\\n\\nThe results were good - on 4 core (8 threads) CPU it runs 4 times faster:\\n\\n```\\ntime ./calculate_average_dg2.sh \\nreal    0m4,992s\\nuser    0m33,384s\\nsys     0m1,207s\\n```\\n\\nSpliterators performance is good because of data locality that it preserves. Processors have limited amount of L1/L2/L3 caches so if you don\'t keep the data close to computation then this translates to cache missess. So best way is to perform operations \\"semi-sequentially\\" just like the spliterators.\\n\\nBut... Streams create threads dynamically and this means you can\'t partition the data into well known parts. So the access has to be synchronized in one place and this one big problem here.\\n\\nIt would be the end of the story if I didn\'t check the leaderboard on competitions github repo. There were speedups as fast as astonishing 32x times faster. So there is much to analyse. Below are the most important optimizations I tried in my playground.\\n\\n## Optimizations\\n\\n### O1. Reading longs instead of bytes\\n\\nYou may recall that everything is represented in JVM as an int. This means that even a byte is in fact and 4-byte int. I\'m not sure if JVM doesn\'t do some kind of compression of byte[] array. If not that would mean the data is actually 4x times larager. So it is wasteful to search newline byte by byte. The trick is to load 8 bytes at once and find newline in whole batch.\\n\\n```\\n long word = buffer.getLong();\\n\\nlong match = word ^ 0x0a0a0a0a0a0a0a0aL;\\nlong line = (match - 0x0101010101010101L) & (~match & 0x8080808080808080L);\\n\\nif (line == 0) {\\n    i += 8;\\n    continue;\\n}\\n\\nnext = i + (Long.numberOfTrailingZeros(line) >>> 3) + 1;\\n```\\n\\nXor will zero the byte that has newline (0x0a). Then we cause underflow which sets leading bit to 1. And then we zero out everthing else. From the leading bit we can get offset of newline in 8-byte pack.\\n\\n### O2. Represent measurements as int / skip parsing\\n\\nWe can take advantage of the data format: floats have only one decimal point and temperatures are not greater or lesser than 100 points for sure. So the idea is to parse the number manually and multiply it by 10 so that it is the integer.\\n\\nThis is not huge improvement but still. Actually big improvement is skipping Double.parseDouble as we can discard uncessary complexity like long mantissa and exponent part.\\n\\n### O3. Defer creating of String\\n\\nStrings are a little bit heavy because they are not underlied by byte[] but char[]. So each time we create a string we encode it and according to the charset. We can defer creating strings until they are needed for printing. But we have to find some other key for the hashmap.\\n\\nCopying byte array is much faster and this is what I used.\\n\\n### O4. Faster hashes\\n\\nFaster non-cryptographic hash is needed. One suitable algorithm for this task is FNV. It is simple enought to implement and run fast.\\n\\n\\n\\n```\\n@Override\\npublic int hashCode() {\\n    long hash = 0x811C9DC5;\\n    long prime = 0x01000193;\\n\\n    for (byte b : bytes) {\\n        hash = hash ^ b;\\n        hash *= prime;\\n    }\\n\\n    return (int) hash;\\n}\\n```\\n\\n### O5. Skipping synchronization\\n\\nWith such huge load synchronization is too slow. It is fine from more coarse grained control flow, but 100 million is just too much. So optimization is to use separate threads each with separate hashmap. Then merge hashmaps after the work is done. As number of hashmaps will be several orders of magnitude less than rows then merging time will be negligible.\\n\\n### O6. Off heap memory\\n\\nWe could use Unsafe to skip array bounds checking for example. But much better way is avoid allocating data on the heap alltogether. With mapping file chunks directly to memory we can easily achieve that. We get MemorySegment and invoke methods which are \\"native\\" to get fast access.\\n\\n### 07. Aligning array address to the multitude of 8 bytes\\n\\n\\nThere is performance penalty for accessing unaligned data, which has to be padded either way. So we can remap all the data in segment once and then enjoy the benefits of reading values in multitudes of 8-bytes (longs).\\n\\n### Final version\\n\\nThe final version uses all above techniques and gets decent peformance. The file is split into 100MB chunks which are memory mapped by each thread. They are executed in form of ForkJoinPool task and run on common pool which has concurrency level same as numer of threads.\\n\\n```\\nclass Measurement {\\n    public int min = Integer.MAX_VALUE;\\n    public int max = Integer.MIN_VALUE;\\n    public int sum = 0;\\n    public int count = 0;\\n\\n    public Measurement(int value) {\\n        this.min = value;\\n        this.max = value;\\n        this.sum = value;\\n        this.count = 1;\\n    }\\n\\n    @Override\\n    public String toString() {\\n        return round(min / 10.0) + \\"/\\" + round(max / 10.0) + \\"/\\" + round((sum / 10.0) / count);\\n    }\\n\\n    private static double round(double value) {\\n        return Math.round(value * 10.0) / 10.0;\\n    }\\n\\n}\\n\\nclass FastKey implements Comparable<FastKey> {\\n    private byte[] bytes;\\n    private int hash;\\n\\n    private static final Charset charset = Charset.forName(\\"UTF-8\\");\\n\\n    public FastKey(MemorySegment segment) {\\n        bytes = segment.toArray(ValueLayout.OfByte.JAVA_BYTE);\\n\\n        long hash = 0x811C9DC5;\\n        long prime = 0x01000193;\\n\\n        for (byte b : bytes) {\\n            hash = hash ^ b;\\n            hash *= prime;\\n        }\\n\\n        this.hash = (int) hash;\\n    }\\n\\n    @Override\\n    public int hashCode() {\\n        return this.hash;\\n    }\\n\\n    @Override\\n    public boolean equals(Object obj) {\\n        if (obj == this) {\\n            return true;\\n        }\\n\\n        if (!(obj instanceof FastKey)) {\\n            return false;\\n        }\\n\\n        return Arrays.equals(bytes, ((FastKey) obj).bytes);\\n    }\\n\\n    @Override\\n    public int compareTo(FastKey o) {\\n        return Arrays.compare(bytes, o.bytes);\\n    }\\n\\n    @Override\\n    public String toString() {\\n        return new String(bytes, charset);\\n    }\\n}\\n\\nclass ComputeMeasurementsPartTask implements Callable<Map<FastKey, Measurement>> {\\n\\n    private int start;\\n    private int end;\\n    private static final Charset charset = Charset.forName(\\"UTF-8\\");\\n    private MemorySegment segment;\\n\\n    private byte getByte(int index) {\\n        return segment.get(ValueLayout.OfByte.JAVA_BYTE, index);\\n    }\\n\\n    private long getLong(int index) {\\n        return segment.get(ValueLayout.OfLong.JAVA_LONG, index);\\n    }\\n\\n    public ComputeMeasurementsPartTask(int start, int end, int limit, FileChannel channel) throws IOException {\\n\\n        this.segment = channel.map(FileChannel.MapMode.READ_ONLY, start, limit - start, Arena.global());\\n        int s = start, s2 = start, e = end;\\n\\n        if (s != 0) {\\n            while (getByte(s - s2) != 0x0a) {\\n                s++;\\n            }\\n            s++;\\n        }\\n\\n        while (e < limit && getByte(e - s2) != 0x0a) {\\n            e++;\\n        }\\n\\n        int prefix = s % 8;\\n        MemorySegment padded = Arena.global().allocate(e - s + prefix);\\n        padded.asSlice(prefix).copyFrom(segment.asSlice(s - s2, e - s));\\n\\n        this.segment = padded;\\n        this.start = prefix;\\n        this.end = e - s + prefix;\\n    }\\n\\n    private void doActualWork(int start, int end, Map<FastKey, Measurement> measurements) {\\n\\n        int splitIndex = start;\\n        while (getByte(splitIndex) != 0x3b) {\\n            splitIndex++;\\n        }\\n\\n        var key = new FastKey(segment.asSlice(start, splitIndex - start));\\n\\n        boolean negative = false;\\n        int ind = splitIndex + 1;\\n\\n        if (getByte(ind) == (byte) \'-\') {\\n            negative = true;\\n            ind++;\\n        }\\n\\n        int v = 0;\\n\\n        if (end - ind == 4) {\\n            v = v * 10 + getByte(ind++) - \'0\';\\n            v = v * 10 + getByte(ind++) - \'0\';\\n            ind++; // \'.\'\\n            v = v * 10 + getByte(ind) - \'0\';\\n        }\\n        else {\\n            v = getByte(ind++) - \'0\';\\n            ind++; // \'.\'\\n            v = v * 10 + getByte(ind) - \'0\';\\n        }\\n\\n        int val = negative ? -v : v;\\n\\n        var _v = measurements.get(key);\\n\\n        if (_v != null) {\\n            if (val < _v.min) {\\n                _v.min = val;\\n            }\\n\\n            if (val > _v.max) {\\n                _v.max = val;\\n            }\\n\\n            _v.sum += val;\\n            _v.count++;\\n        }\\n        else {\\n            measurements.put(key, new Measurement(val));\\n        }\\n    }\\n\\n    @Override\\n    public Map<FastKey, Measurement> call() throws Exception {\\n        var measurements = new HashMap<FastKey, Measurement>();\\n\\n        int prev = start;\\n        for (int i = 0; i < end; i += 8) {\\n            if (i + 8 < end) {\\n                long word = getLong(i);\\n\\n                long match = word ^ 0x0a0a0a0a0a0a0a0aL;\\n                long line = (match - 0x0101010101010101L) & (~match & 0x8080808080808080L);\\n\\n                if (line == 0) {\\n                    i += 8;\\n                    continue;\\n                }\\n                int next = i + (Long.numberOfTrailingZeros(line) >>> 3);\\n                doActualWork(prev, next, measurements);\\n                prev = next + 1;\\n            }\\n            else {\\n                doActualWork(prev, end, measurements);\\n            }\\n\\n        }\\n\\n        return measurements;\\n    }\\n}\\n\\npublic class CalculateAverage_dg {\\n\\n    public static final String FILE = \\"./measurements.txt\\";\\n\\n    public static void main(String[] args) throws Exception {\\n\\n        FileChannel channel = FileChannel.open(Path.of(CalculateAverage_dg.FILE), StandardOpenOption.READ);\\n\\n        var parts = new ArrayList<Map<FastKey, Measurement>>();\\n        var futures = new ArrayList<Future<Map<FastKey, Measurement>>>();\\n\\n        int fileSize = (int) Files.size(Path.of(FILE));\\n        int chunkSize = 20 * 1024 * 1024;\\n        int noOfThreads = fileSize / chunkSize;\\n\\n        for (int i = 0; i < noOfThreads; i++) {\\n            int start = i * chunkSize;\\n            int end = Math.min((i + 1) * chunkSize, fileSize);\\n            int limit = Math.min((i + 1) * chunkSize + 1024, fileSize);\\n\\n            var task = new ComputeMeasurementsPartTask(start, end, limit, channel);\\n            futures.add(ForkJoinPool.commonPool().submit(task));\\n        }\\n\\n        for (var future : futures) {\\n            parts.add(future.get());\\n        }\\n\\n        var measurements = parts.stream().flatMap(map -> map.entrySet().stream())\\n                .collect(\\n                        Collectors.toMap(\\n                                Map.Entry::getKey,\\n                                Map.Entry::getValue,\\n                                (e1, e2) -> {\\n                                    e1.min = Integer.min(e1.min, e2.min);\\n                                    e1.max = Integer.max(e1.max, e2.max);\\n\\n                                    e1.sum += e2.sum;\\n                                    e1.count += e2.count;\\n\\n                                    return e1;\\n                                }));\\n\\n        var sorted = new TreeMap<>(measurements);\\n\\n        System.out.println(sorted);\\n\\n    }\\n}\\n```\\n\\nThe time is impressive:\\n\\n```\\ntime ./calculate_average_dg.sh \\n\\nreal    0m1,465s\\nuser    0m8,505s\\nsys     0m0,526s\\n```\\n\\nFor comparison this is the time of top solution from the leaderboard.\\n\\n```\\ntime ./calculate_average_thomaswue.sh \\n\\nreal    0m0,794s\\nuser    0m5,032s\\nsys     0m0,199s\\n```\\n\\nSo it is \\"only\\" two times slower than the best. I don\'t know how would that translate to the leaderboard. But this guy is actually founder of GraalVM so he definitely knows what he is doing.\\n\\n## Conclusion\\n\\n* ConcurrentHashMap is suprisingly fast, without good background it would be hard to write something faster\\n* Off-heap memory gives considerable performance boost\\n* Reading file as longs is a nice trick that I didn\'t know up to date"},{"id":"/2024/10/26/","metadata":{"permalink":"/blog/2024/10/26/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-10-26.md","source":"@site/blog/2024-10-26.md","title":"Topology graphs are important (and fun)","description":"Recently I watched the presentation from Microsoft about Radius. They developed the tool to foster the collaboration between devops and developers. In nutshell devops create \\"recipies\\" in Bicep or Terraform and developers use them to deploy the application. Seems cool.","date":"2024-10-26T00:00:00.000Z","tags":[{"inline":true,"label":"k8s","permalink":"/blog/tags/k-8-s"},{"inline":true,"label":"openshift","permalink":"/blog/tags/openshift"}],"readingTime":2.52,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Topology graphs are important (and fun)","tags":["k8s","openshift"]},"unlisted":false,"prevItem":{"title":"1BRC Challenge","permalink":"/blog/2024/10/29/"},"nextItem":{"title":"Writing own Kubernetes operator in Java","permalink":"/blog/2024/10/4/"}},"content":"Recently I watched the presentation from Microsoft about Radius. They developed the tool to foster the collaboration between devops and developers. In nutshell devops create \\"recipies\\" in Bicep or Terraform and developers use them to deploy the application. Seems cool.\\n\\n\x3c!-- truncate --\x3e\\n\\nHowever this is not the topic for this article. They mentioned the idea that instantly clicked in my head. That is - we waste so much of our time when interacting with kubernetes. Imagine you want to learn why app is not working and you know that it depends on \\"downstream\\" app. So you start with the deployment, it\'s fine, then you go to the code and find out the k8s service name that it\'s using. Then by labels or simply by name you find the dependent deployment that is failing. So we had 3 context switches. What about the dependencies that are > 2 links deep?\\n\\nExactly this is just too much work. There is a better way to handle things. We draw graph where deployments are nodes and edges are dependencies. This was briefly mentioned in the presentation however it was not main topic. But the concept is not new. Openshift as much as I was able to reasearch had it 5 years ago already.\\n\\n## Topology view in OpenShift\\n\\nOne picture worth thousand of words:\\n\\n![](/img/scshot-1.png)\\n\\nWhat if the postres went down? Then it would look like this:\\n\\n![](/img/scshot-2.png)\\n\\nI mean not exactly, it would be rounded in red circle, but you get the picture. You can instantly see.\\n\\nWhat is more you can view its code with Eclipse Che by deploying own workspace. Committing changes with git is whole different story...\\n\\n### What\'s under the hood\\n\\nI clicked away all this in OpenShift but the real use case would be deploying apps with helm. So how is it done?\\n\\nAs it turns out these are just plain labels and annotations.\\n\\nFirst the simple stuff:\\n\\n![](/img/scshot-3.png)\\n\\n**How would you get the \\"Go\\" picture? Very simple:**\\n\\n```\\nkubectl label deployments go-basic app.openshift.io/runtime=golang\\n```\\n\\n**How would you group applications together?**\\n\\n```\\nkubectl label deployments go-basic app.kubernetes.io/part-of=sample-app\\nkubectl label deployments frontend app.kubernetes.io/part-of=sample-app\\n```\\n\\n**How would you get the dependency-arrows?**\\n\\n```\\nkubectl annotate deployments frontend \'app.openshift.io/connects-to=[{\\"apiVersion\\":\\"apps/v1\\",\\"kind\\":\\"Deployment\\",\\"name\\":\\"go-basic\\"}\\n```\\n\\nSo there is no magic here. These are just plain labels and annotations and the beauty is that they don\'t even interfere with your workflow.\\n\\n## Kubernetes standard labels/annotations\\n\\nKubernetes standard labels/annotations\\n\\n**app.kubernetes.io/name**\\n\\nThis is the name of the application\\n\\n**app.kubernetes.io/instance**\\n\\nThis is the name of the replica/instance in case you have many applications. You can think of it as blueprint - object pattern.\\n\\n**app.kubernetes.io/version**\\n\\nNo explanation needed\\n\\n**app.kubernetes.io/component**\\n\\nNot sure what this does, but probably some categorization backend versus frontend.\\n\\n**app.kubernetes.io/part-of**\\n\\nWe have seen it that it groups apps in OpenShift.\\n\\n**app.kubernetes.io/managed-by**\\n\\nI would recommend avoiding it, as it interferes with OpenShift topology rendering.\\n\\nWhat you could do in couple of minutes, you can do in seconds. Viewing graphs instead of list of resources enables you to debug the system faster and hassle free."},{"id":"/2024/10/4/","metadata":{"permalink":"/blog/2024/10/4/","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2024-10-4.md","source":"@site/blog/2024-10-4.md","title":"Writing own Kubernetes operator in Java","description":"Operators introduced by CoreOS in 2016 are now considered early majority on Categories of Adopters scale.","date":"2024-10-04T00:00:00.000Z","tags":[{"inline":true,"label":"k8s","permalink":"/blog/tags/k-8-s"},{"inline":true,"label":"java","permalink":"/blog/tags/java"},{"inline":true,"label":"devops","permalink":"/blog/tags/devops"}],"readingTime":5.865,"hasTruncateMarker":true,"authors":[],"frontMatter":{"title":"Writing own Kubernetes operator in Java","tags":["k8s","java","devops"]},"unlisted":false,"prevItem":{"title":"Topology graphs are important (and fun)","permalink":"/blog/2024/10/26/"}},"content":"Operators introduced by [CoreOS](https://www.redhat.com/en/blog/introducing-operators-putting-operational-knowledge-into-software) in 2016 are now considered early majority on Categories of Adopters scale.\\n\\n\x3c!-- truncate --\x3e\\n\\nIt means that the technology is becoming pretty mainstream. They use Kubernetes Server API to enforce some operational patterns for a deployment of application. This way application developers can translate the \\"domain\\" knowledge to Kubernetes land.\\n\\nI experimented with them lately, but not enough skills of Go discouraged me at first. But then I realised that I can try to write an Operator in my \\"mother tonque\\" - Java. In this post I will share couple of reflections on the topic.\\n\\n**TLDR** [Here](https://github.com/vulture-dominiczek/operator-hello-world) is the hello world I wrote.\\n\\n## Anatomy of an operator\\n\\nFirst and most important of all, [Operator](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/) is an extension to Kubernetes. The core idea is to capture what human operator/admin would normally do and encode it in software.\\n\\nAt some point when Kubernetes was adopted by more and more users there occured an obvious need to extend it\'s API. A **resource** in Kubernetes is an endpoint that groups several API objects. That is for example pods resource groups pods and allows actions like **get, delete, patch** and so on to be acted on them.\\n\\nBut operators are extension to the system so there must be a way to represent their objectives Kubernetes (declarative) proper way. And there is - [Custom Resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/).\\n\\nWhenever you write a new Operator, you will want to create custom resource for it. As soon as you register your custom resouce in Kubernetes it will start to serve it from API Server and all actions like get, delete and others will be available. And then your Operator will monitor this new resource and take appropirate actions when desired.\\n\\n#### Control loop\\n\\nDemystification no. 1 is that Operators are nothing new. Kubernetes is driven internally by Controllers. From the definition they implement **control loop**.\\n\\nFunny thing, the term is actually taken from robotics and it is *nonterminating loop that corrects the state of the system*. What does it mean? For example one simple controller could watch that the certain pod is present in system all the times. Whenever it detects that pod is down it will take steps to restart it and correct the \\"glitch\\".\\n\\nNow, operators are basically the same thing, but as a form of extension. Kubernetes is very permissive when it comes to it\'s API. So as long as you can reach the Server API (and have rights) then you can do any thing with it. So here is the thing -- operator is a custom controller. But: when it boils down to it\'s core -- it\'s just a pod! Like any other. I mean you will probably want to add replication, leader election and so on, but this is still a piece of software with an access to Kubernetes API.\\n\\n![](/img/public_cloud-containers_orchestration-managed_kubernetes-installing-go-operator-images-operator.png)\\n\\nTo sum up: operator watches events from the server (it can), or reads the status of current resources and matches that with whats specified in CRD. If there is a mismatch it does **reconcillation**. This is just another term for control loop. But outside Kubernetes core API.\\n\\n#### Kubernetes Client in Java\\n\\nGo to solution when dealing in Kubernetes in Java is Fabric8 client. You also want to watch this presentation.\\n\\nYou init the client very easily:\\n\\n```java\\nKubernetesClient client = new KubernetesClientBuilder().build();\\n```\\n\\nAnd then there is a matter of reading the api and ivoking appropirate methods:\\n\\n```java\\nvar currentServingDeploymentNullable = client.apps().deployments().inNamespace(\\"default\\")\\n                    .withName(\\"web-serving-app-deployment\\").get();\\nvar currentServingDeployment = Optional.ofNullable(currentServingDeploymentNullable);\\n```\\n\\n#### Informers\\n\\nSo, the basic idea is to forever run the control loop, sleep couple of seconds and do everthing again. But this is not the best way to do this. Ideally you would want to only do work when something changes. Actually you can do this easily as well.\\n\\nKubernetes has a notion of informers. These are WebSocket connections-subscriptions to changes of particular resources. So for example, you could watch all the pods in the namespace for changes and get informed when anything changes at all.\\n\\nThis leads to following solution - in the end of reconcillation block on the monitor:\\n\\n```java\\nprivate static Object changes = new Object();\\n...\\nsynchronized (changes) {\\n    changes.wait();\\n}\\n```\\n\\nWhenever something changes our informer will let us unblock the control loop:\\n\\n```java\\nvar handler = new GenericResourceEventHandler<>(update -> {\\n    synchronized (changes) {\\n        changes.notifyAll();\\n    }\\n});\\n\\ncrdClient.inform(handler).start();\\n```\\n\\nThe Callback is a little bit bloated so I abstracted it away like so:\\n\\n```java\\n\\n\\npublic class GenericResourceEventHandler<T> implements ResourceEventHandler<T> {\\n\\n    private final Consumer<T> handler;\\n\\n    public GenericResourceEventHandler(Consumer<T> handler) {\\n        this.handler = handler;\\n    }\\n\\n\\n    @Override\\n    public void onAdd(T obj) {\\n        this.handler.accept(obj);\\n    }\\n\\n    @Override\\n    public void onUpdate(T oldObj, T newObj) {\\n        this.handler.accept(newObj);\\n    }\\n\\n    @Override\\n    public void onDelete(T obj, boolean deletedFinalStateUnknown) {\\n        this.handler.accept(null);\\n    }\\n}\\n```\\n\\n#### Deployment\\n\\nSo when you are done with the implementation you will probably want to deploy the operator... I wrote it as the Spring Boot application and some interesting stuff happened on the way.\\n\\n#### Tip 1\\n\\nYou will want to have a private repository, and I chose GHCR. You can set it up and download the passcode. Then create secret for kubernetes:\\n\\n```bash\\nkubectl create secret docker-registry regcred \\\\\\n  --docker-server=ghcr.io \\\\\\n  --docker-username=dgawlik \\\\\\n  --docker-password=$GITHUB_TOKEN\\n```\\n\\n#### Tip 2\\n\\nYou have to create CRD of course. Actually fabric8 client got you covered:\\n\\n```\\n <dependency>\\n    <groupId>io.fabric8</groupId>\\n    <artifactId>kubernetes-client</artifactId>\\n    <version>6.13.4</version>\\n</dependency>\\n<dependency>\\n    <groupId>io.fabric8</groupId>\\n    <artifactId>crd-generator-apt</artifactId>\\n    <version>6.13.4</version>\\n    <scope>provided</scope>\\n</dependency>\\n```\\n\\nWhenever you create CRD classes it will generate the CRD manifest so you can kubectl apply it. So for example:\\n\\n```java\\n\\n@Group(\\"com.github.webserving\\")\\n@Version(\\"v1alpha1\\")\\n@ShortNames(\\"websrv\\")\\npublic class WebServingResource extends CustomResource<WebServingSpec, WebServingStatus> implements Namespaced {\\n}\\n\\npublic record WebServingSpec(String page1, String page2) {\\n}\\n\\npublic record WebServingStatus (String status) {\\n}\\n```\\n\\n#### Tip 3\\n\\nYou will want to create native images with GraalVm to speed things up. If you don\'t have a lot of memory then you can tradeoff the quality of binary for building time/resources.\\n\\n```\\n<build>\\n    <plugins>\\n        <plugin>\\n            <groupId>org.graalvm.buildtools</groupId>\\n            <artifactId>native-maven-plugin</artifactId>\\n            <configuration>\\n                <buildArgs>\\n                    <buildArg>-Ob</buildArg>\\n                </buildArgs>\\n            </configuration>\\n        </plugin>\\n        <plugin>\\n            <groupId>org.springframework.boot</groupId>\\n            <artifactId>spring-boot-maven-plugin</artifactId>\\n            <configuration>\\n                <image>\\n                    <publish>true</publish>\\n                    <builder>paketobuildpacks/builder-jammy-full:latest</builder>\\n                    <name>ghcr.io/dgawlik/webpage-serving:1.0.5</name>\\n                    <env>\\n                        <BP_JVM_VERSION>21</BP_JVM_VERSION>\\n                    </env>\\n                </image>\\n                <docker>\\n                    <publishRegistry>\\n                        <url>https://ghcr.io/dgawlik</url>\\n                        <username>dgawlik</username>\\n                        <password>${env.GITHUB_TOKEN}</password>\\n                    </publishRegistry>\\n                </docker>\\n            </configuration>\\n        </plugin>\\n    </plugins>\\n</build>\\n```\\n\\nAnd second when you set publish property to true, the package step will automatically push the image to your repositry.\\n\\n\\nThe third - you pass -Ob parameter to GraalVM. This will insturct the runtime to do fastest, cheapest build possible. And of course - BP_JVM_VERSION has to be java of your project or things will not work.\\n\\nAnd last thing -- if you want debug the container, you will have to choose paketobuildpacks/builder-jammy-full:latest as other buildpacks don\'t include shell (shame).\\n\\n## Conclusion\\n\\nI haven\'t covered everything, but everything else is in the repo. The repo is proof of concept that operators in Java are not complicated at all. I would say that even they are easier than in Go. So in the repo you will find following:\\n\\n* spring-boot static server\\n* operator that watches CRD and mounts config maps in server so that it can serve the websites from the CRD\\n\\nSo this basically is Operator hello world."}]}}')}}]);